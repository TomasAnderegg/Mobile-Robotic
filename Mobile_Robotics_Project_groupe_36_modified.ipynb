{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, we tried to divide the tasks as follows: Victor handled the Vision part, Hoang worked on Path Planning and Motion Control, while Tomas focused on filtering, kidnapping, and local avoidance. As we progressed with the project, we quickly realized there were different ways to implement things. Unfortunately, due to the robot's imprecise sensors, we encountered many difficulties with both motion control and filtering. Nevertheless, we are proud to present the work, which cost us a lot of time and sleepless nights, even though we had hoped for a perfect result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Node d03d4c82-c7d9-461b-9284-191eb97b1e15"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tdmclient import ClientAsync\n",
    "import cv2\n",
    "from scipy.spatial import distance as dist \n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "import math\n",
    "%matplotlib qt\n",
    "\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import numpy as np\n",
    "client = ClientAsync()\n",
    "node = await client.wait_for_node()\n",
    "\n",
    "await node.lock()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# FILTERING\n",
    "THYMIO_MMS = 0.12 # Thymio's unit to mm/s\n",
    "D_BASELINE = 100 # mm\n",
    "DELTA_T = 0.5 # s\n",
    "SIZE_PLOT = 110\n",
    "KIDNAP_THRESHOLD = 5\n",
    "\n",
    "KIDNAPPING_THRESHOLD = 3\n",
    "\n",
    "# MOTION CONTROL\n",
    "SUCCESS_THRESHOLD = 40 \n",
    "VELOCITY_THRESHOLD = 75 \n",
    "\n",
    "# VISION\n",
    "SCALING_FACTOR = 20/3\n",
    "\n",
    "# color filters\n",
    "HSV_RED_MIN = np.array([165, 30, 30])\n",
    "HSV_RED_MAX = np.array([180, 255, 255])\n",
    "\n",
    "HSV_ORANGE_MIN = np.array([0, 30, 30])\n",
    "HSV_ORANGE_MAX = np.array([15, 255, 255])\n",
    "\n",
    "HSV_GREEN_MIN = np.array([65, 40, 40])\n",
    "HSV_GREEN_MAX = np.array([85, 255, 255])\n",
    "\n",
    "HSV_BLUE_MIN = np.array([85, 40, 40])\n",
    "HSV_BLUE_MAX = np.array([115, 255, 255])\n",
    "\n",
    "TOLERANCE_HSV = np.array([8, 100, 100])\n",
    "\n",
    "# change those values with the camera \n",
    "RED_BGR = np.array([115, 95, 194], dtype = np.uint8)\n",
    "BLUE_BGR = np.array([134, 73, 12], dtype= np.uint8)\n",
    "GREEN_BGR = np.array([29, 72, 56], dtype = np.uint8)\n",
    "ORANGE_BGR = np.array([134, 73, 12], dtype = np.uint8)\n",
    "\n",
    "# camera and map parameters\n",
    "CAMERA_WIDTH = 1024\n",
    "CAMERA_HEIGHT = 768\n",
    "BIRD_WIDTH = 660\n",
    "BIRD_HEIGHT = 492\n",
    "MAP_WIDTH = 330\n",
    "MAP_HEIGHT = 246\n",
    "REAL_WIDTH = 1100\n",
    "REAL_HEIGHT = 820\n",
    "CAMERA_CORNERS = [[0, 0], [CAMERA_WIDTH, 0], [CAMERA_WIDTH, CAMERA_HEIGHT], [0, CAMERA_HEIGHT]]\n",
    "BIRD_CORNERS = [[0, 0], [BIRD_WIDTH, 0], [BIRD_WIDTH, BIRD_HEIGHT], [0, BIRD_HEIGHT]]\n",
    "MAP_CORNERS = np.array([[0, 0], [MAP_WIDTH, 0], [MAP_WIDTH, MAP_HEIGHT], [0, MAP_HEIGHT]], dtype=np.float32)\n",
    "\n",
    "# matching thresholds\n",
    "RES_THRESHOLD_BLACK = 0.84\n",
    "RES_THRESHOLD_RED = 0.4\n",
    "RES_THRESHOLD_GREEN = 0.3\n",
    "\n",
    "# binary thresholds\n",
    "BINARY_THRESHOLD_BLACK = 27\n",
    "BINARY_THRESHOLD_RED = 27\n",
    "BINARY_THRESHOLD_GREEN = 50\n",
    "BINARY_THRESHOLD_BLUE = 20\n",
    "\n",
    "OVERLAP_THRESHOLD = 0\n",
    "THYMIO_WIDTH = 29\n",
    "NOISE_WIDTH = 3\n",
    "TEMP_CENTER_Y = 53\n",
    "TEMP_CENTER_X = 69\n",
    "\n",
    "OBSTACLE_MAP_VALUE = -1\n",
    "BACKGROUND_MAP_VALUE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#templates\n",
    "template_cross = cv2.imread('/Users/tjga9/Documents/Tomas/EPFL/MA1/Basic of Mobile Robotics/Control your Thymio in Python/Control your Thymio in Python/template_cross_dll.jpg')\n",
    "\n",
    "method = eval('cv2.TM_CCORR_NORMED')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_plot():\n",
    "    fig,ax = plt.subplots()\n",
    "    line, = ax.plot([], [], '--', label=\"Position with odometry\", color =\"black\")\n",
    "    ax.set_xlim(-SIZE_PLOT, SIZE_PLOT+100)\n",
    "    ax.set_ylim(-SIZE_PLOT, SIZE_PLOT)\n",
    "    ax.set_title(\"Real time Kalmann plot\")\n",
    "    ax.set_xlabel(\"x\")\n",
    "    ax.set_ylabel(\"y\")\n",
    "    ax.legend()\n",
    "    ax.minorticks_on()\n",
    "    ax.grid(which='major', color=\"0.75\", linestyle='-')\n",
    "    #ax.grid(which='minor', color=\"0.75\", linestyle='--')\n",
    "    plt.rcParams.update({\n",
    "        \"text.usetex\": False,\n",
    "        \"font.family\": \"sans-serif\"\n",
    "    })\n",
    "\n",
    "    return fig, ax, line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_map(map_grid, path, start, goal):\n",
    "    cmap = ListedColormap(['white', 'black', 'blue', 'green', 'red', 'grey'])\n",
    "    map_display = np.zeros_like(map_grid, dtype=object)\n",
    "\n",
    "    # Assign colors based on the map grid values\n",
    "    map_display[map_grid == -1] = 'black'  # Obstacles\n",
    "    map_display[map_grid == 0] = 'white'   # Free space\n",
    "\n",
    "    # for position in explored:\n",
    "    #     if map_display[tuple(position)] == 'white':\n",
    "    #         map_display[tuple(position)] = 'grey'  # Explored cells\n",
    "\n",
    "    # Visualize the path\n",
    "    for position in path:\n",
    "        if map_display[position[0], position[1]] in ['white', 'grey']:\n",
    "            map_display[position[0], position[1]] = 'blue'  # Path\n",
    "\n",
    "    # map_display[5, 3] = 'yellow' # Weighted cell\n",
    "    map_display[start[0], start[1]] = 'green'  # Start\n",
    "    map_display[goal[0], goal[1]] = 'red'      # Goal\n",
    "\n",
    "    # Convert color names to numbers for plotting\n",
    "    color_mapping = {'white': 0, 'black': 1, 'blue': 2, 'green': 3, 'red': 4, 'grey': 5}\n",
    "    map_numeric_display = np.vectorize(color_mapping.get)(map_display)\n",
    "    fig, ax = plt.subplots(figsize=(6, 7))\n",
    "    ax.imshow(map_numeric_display, cmap=cmap)\n",
    "    ax.set_xticks(np.arange(-0.5, map_grid.shape[1], 1), minor=True)\n",
    "    ax.set_yticks(np.arange(-0.5, map_grid.shape[0], 1), minor=True)\n",
    "    ax.grid(which='minor', color='gray', linestyle='-', linewidth=0.5)\n",
    "    ax.tick_params(which='both', bottom=False, left=False, labelbottom=False, labelleft=False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thymio_transform_pose(pose, start):\n",
    "    \"\"\"\n",
    "    Transform the position from vision reference (y-axis right-sided, x-axis downward) to\n",
    "    new reference with origin at the start position (y-axis upward, x-axis right-sided).\n",
    "    \n",
    "    Parameters: \n",
    "    pose (tuple): The input position in vision reference. \n",
    "    start (tuple): The input start position.\n",
    "    \n",
    "    Return:\n",
    "    list: The ouput path as a list of tuples (sub-goals) in the new reference at the start position.\n",
    "    \"\"\"\n",
    "    \n",
    "    start_x, start_y = start\n",
    "    \n",
    "    # transform the reference\n",
    "    new_x = (pose[1] - start_y)\n",
    "    new_y = -(pose[0] - start_x)\n",
    "    \n",
    "    return new_x, new_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_angle(angle):\n",
    "    \"\"\"\n",
    "    Normalize the given angle to range [-π, π].\n",
    "    \n",
    "    Parameters:\n",
    "    angle (float): The input angle in radian.\n",
    "    \n",
    "    Returns:\n",
    "    float: The output normalized angle to [-π, π] in radian.\n",
    "    \"\"\"\n",
    "    \n",
    "    normalized_angle = (angle + np.pi) % (2 * np.pi) - np.pi\n",
    "    \n",
    "    return normalized_angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_angle_full(angle_radians):\n",
    "    # Normalization between -2pi and 2pi\n",
    "    two_pi = 2*np.pi\n",
    "    if angle_radians >= two_pi:\n",
    "        angle_radians -= two_pi\n",
    "    elif angle_radians < -two_pi:\n",
    "        angle_radians += two_pi\n",
    "    return angle_radians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.rad2deg(normalize_angle(np.deg2rad(445))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def motors(left, right):\n",
    "    return {\n",
    "        \"motor.left.target\": [left],\n",
    "        \"motor.right.target\": [right],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vision\n",
    "======\n",
    "\n",
    "There are six things to do with the vision : \\\n",
    "_Detect if the camera is obstructed\\\n",
    "_Warp the camera image to only keep the map and make it as if we are exactly on top of it\\\n",
    "_detect the obstacles to create a map\\\n",
    "_detect the goal\\\n",
    "_detect the pose of the thymio, with position and angle\\\n",
    "_show the global path and the kalman filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use multiple libraries such as numpy and mostly opencv2 functions. 2 functions have been taken from pyImageSearch :\n",
    "https://pyimagesearch.com/2015/02/16/faster-non-maximum-suppression-python/ <br>\n",
    "https://pyimagesearch.com/2016/03/21/ordering-coordinates-clockwise-with-python-and-opencv/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_corners = CAMERA_CORNERS #variable that will be updated each time\n",
    "is_cam_obstructed = False\n",
    "map = np.zeros((BIRD_HEIGHT, BIRD_WIDTH))\n",
    "bird_image = np.zeros((BIRD_HEIGHT, BIRD_WIDTH))\n",
    "hsv_img = np.zeros((BIRD_HEIGHT, BIRD_WIDTH, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def band_pass_hsv(bgr_color):\n",
    "\t\"\"\"given a color in bgr colorspace, returns the min and max values for the bandpass filter\"\"\"\n",
    "\thsv_color = cv2.cvtColor(bgr_color.reshape(1, 1, 3), cv2.COLOR_BGR2HSV)[0][0]\n",
    "\thsv_min = hsv_color - TOLERANCE_HSV\n",
    "\thsv_max = hsv_color + TOLERANCE_HSV\n",
    "\n",
    "\treturn hsv_min, hsv_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_to_hsv(image):\n",
    "\t\"\"\"Filters the image from BGR colorspace to HSV colorspace\"\"\"\n",
    "\tfiltered_img = cv2.GaussianBlur(image,(5,5),1)\n",
    "    \n",
    "    # Convert BGR to HSV\n",
    "\thsv_img = cv2.cvtColor(filtered_img, cv2.COLOR_BGR2HSV)\n",
    "\treturn hsv_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_to_hsv(image):\n",
    "\t\"\"\"Filters the image from BGR colorspace to HSV colorspace\"\"\"\n",
    "\tfiltered_img = cv2.GaussianBlur(image,(5,5),1)\n",
    "    \n",
    "    # Convert BGR to HSV\n",
    "\thsv_img = cv2.cvtColor(filtered_img, cv2.COLOR_BGR2HSV)\n",
    "\treturn hsv_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_hsv_to_binary(hsv_img, bgr_color, binary_threshold):\n",
    "\t\"\"\"filters the color of an hsv image to a binary image\n",
    "\t| hsv img : image in the hsv colorspace\n",
    "\t| bgr_color : color to filter in the bgr colorspace\n",
    "\t| binary threshold : value btw 0 and 255 for binary filter\"\"\"\n",
    "\thsv_min, hsv_max = band_pass_hsv(bgr_color)\n",
    "\tmask = cv2.inRange(hsv_img, hsv_min, hsv_max)\n",
    "\toutput = cv2.bitwise_and(hsv_img,hsv_img, mask= mask)\n",
    "\tgrayscale = output[:, :, 2]\n",
    "\t#convert to binary\n",
    "\tthreshold = binary_threshold\n",
    "\tret, bw = cv2.threshold(grayscale, threshold, 255, cv2.THRESH_BINARY)\n",
    "\treturn bw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_grayscale(img):\n",
    "\t\"given image in bgr colorspace, convert it to grayscale then convert it to binary\"\n",
    "\t#convert to grayscale both image and template\n",
    "\timg_grayscale = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\t#convert to binary both image and template\n",
    "\tthreshold = BINARY_THRESHOLD_BLACK\n",
    "\tret, image_binary = cv2.threshold(img_grayscale, threshold, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "\treturn image_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_back_coord(pose, start):\n",
    "    #pose is in (x, y) of thymio ref, start is (y, x) in matrix ref, converts to (x,y) to feed to opencv\n",
    "    start_y, start_x = start\n",
    "    new_x = pose[0] + start_x\n",
    "    new_y = -pose[1] + start_y\n",
    "\n",
    "    return np.array([new_x, new_y], dtype= int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_path(image, path, start):\n",
    "    #normalize coordinates ? we project onto an image twice bigger than our coord -> need to double the coord\n",
    "    # print(\"SearchStart\", start)\n",
    "    previous = transform_back_coord(path[0], start)*2\n",
    "    # print(\"previous\", previous)\n",
    "    for i in range(1, len(path)):\n",
    "        # print(\"i\", i)\n",
    "        current = transform_back_coord(path[i], start)*2\n",
    "        # print(\"current\", current)\n",
    "        # print(\"previous\", previous)\n",
    "        cv2.line(image, previous, current, (0, 0, 255), 3)\n",
    "        previous = current\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_kalman(image, mean, spread, start):\n",
    "    \"\"\"Shows the estimation of the kalman filter on the image.\n",
    "    Arguments:\n",
    "        image : image to show the kalman filter on.\n",
    "        mean : mean (x, y, theta) of the kalman filter in mm.\n",
    "        spread : variance matrix of the Kalman Filter.\n",
    "        start : start position in numpy array coordinates.\n",
    "    Returns:\n",
    "        image : annotated image\"\"\"\n",
    "    #mean : result from filter : x, y, theta in pixels with reference of the thymio\n",
    "    #spread : spread in x, spread in y that are in mm\n",
    "    \n",
    "    spread = (int(spread[0, 0]), int(spread[1, 1]))\n",
    "    spread = (spread[0]*3//5, spread[1]*3//5)\n",
    "    angle = np.rad2deg(mean[2])\n",
    "    start = np.array(start)*2 \n",
    "    pose = (mean[0]*2, mean[1]*2) #multiply by 2 bc the mean of thymio is a map position and image shown is twice bigger than the map\n",
    "    if spread[0] == 0:\n",
    "        spread = (1, spread[1])\n",
    "    if spread[1] == 0:\n",
    "        spread = (spread[0], 1)    \n",
    "    center = transform_back_coord(pose, start)\n",
    "    \n",
    "    cv2.ellipse(image, center, spread, angle, 0, 360, (0, 255, 0), 1)\n",
    "    #spread[::-1] because we want variance in y first\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_grayscale(img):\n",
    "\t\"given image in bgr colorspace, convert it to grayscale then convert it to binary\"\n",
    "\t#convert to grayscale both image and template\n",
    "\timg_grayscale = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\t#convert to binary both image and template\n",
    "\tthreshold = BINARY_THRESHOLD_BLACK\n",
    "\tret, image_binary = cv2.threshold(img_grayscale, threshold, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "\treturn image_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_suppression_fast(boxes, overlapThresh):\n",
    "\t\"\"\"Makes sure to only have one set of coordinate per cross to find the corners of the map (from pyImageSearch)\"\"\"\n",
    "\t# if there are no boxes, return an empty list\n",
    "\tif len(boxes) == 0:\n",
    "\t\treturn []\n",
    "\t# if the bounding boxes integers, convert them to floats --\n",
    "\t# this is important since we'll be doing a bunch of divisions\n",
    "\tif boxes.dtype.kind == \"i\":\n",
    "\t\tboxes = boxes.astype(\"float\")\n",
    "\t# initialize the list of picked indexes\t\n",
    "\tpick = []\n",
    "\t# grab the coordinates of the bounding boxes\n",
    "\tx1 = boxes[:,0]\n",
    "\ty1 = boxes[:,1]\n",
    "\tx2 = boxes[:,2]\n",
    "\ty2 = boxes[:,3]\n",
    "\t# compute the area of the bounding boxes and sort the bounding\n",
    "\t# boxes by the bottom-right y-coordinate of the bounding box\n",
    "\tarea = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "\tidxs = np.argsort(y2)\n",
    "\t# keep looping while some indexes still remain in the indexes\n",
    "\t# list\n",
    "\twhile len(idxs) > 0:\n",
    "\t\t# grab the last index in the indexes list and add the\n",
    "\t\t# index value to the list of picked indexes\n",
    "\t\tlast = len(idxs) - 1\n",
    "\t\ti = idxs[last]\n",
    "\t\tpick.append(i)\n",
    "\t\t# find the largest (x, y) coordinates for the start of\n",
    "\t\t# the bounding box and the smallest (x, y) coordinates\n",
    "\t\t# for the end of the bounding box\n",
    "\t\txx1 = np.maximum(x1[i], x1[idxs[:last]])\n",
    "\t\tyy1 = np.maximum(y1[i], y1[idxs[:last]])\n",
    "\t\txx2 = np.minimum(x2[i], x2[idxs[:last]])\n",
    "\t\tyy2 = np.minimum(y2[i], y2[idxs[:last]])\n",
    "\t\t# compute the width and height of the bounding box\n",
    "\t\tw = np.maximum(0, xx2 - xx1 + 1)\n",
    "\t\th = np.maximum(0, yy2 - yy1 + 1)\n",
    "\t\t# compute the ratio of overlap\n",
    "\t\toverlap = (w * h) / area[idxs[:last]]\n",
    "\t\t# delete all indexes from the index list that have\n",
    "\t\tidxs = np.delete(idxs, np.concatenate(([last],\n",
    "\t\t\tnp.where(overlap > overlapThresh)[0])))\n",
    "\t# return only the bounding boxes that were picked using the\n",
    "\t# integer data type\n",
    "\treturn boxes[pick].astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_points(pts):\n",
    "\t\"\"\"Orders points of rectangle so that they are in the order : top_left, top_right, bottom_right, bottom_left (from pyImageSearch)\"\"\"\n",
    "\t# sort the points based on their x-coordinates\n",
    "\txSorted = pts[np.argsort(pts[:, 0]), :]\n",
    "\t# grab the left-most and right-most points from the sorted\n",
    "\t# x-roodinate points\n",
    "\tleftMost = xSorted[:2, :]\n",
    "\trightMost = xSorted[2:, :]\n",
    "\t# now, sort the left-most coordinates according to their\n",
    "\t# y-coordinates so we can grab the top-left and bottom-left\n",
    "\t# points, respectively\n",
    "\tleftMost = leftMost[np.argsort(leftMost[:, 1]), :]\n",
    "\t(tl, bl) = leftMost\n",
    "\t# now that we have the top-left coordinate, use it as an\n",
    "\t# anchor to calculate the Euclidean distance between the\n",
    "\t# top-left and right-most points; by the Pythagorean\n",
    "\t# theorem, the point with the largest distance will be\n",
    "\t# our bottom-right point\n",
    "\tD = dist.cdist(tl[np.newaxis], rightMost, \"euclidean\")[0]\n",
    "\t(br, tr) = rightMost[np.argsort(D)[::-1], :]\n",
    "\t# return the coordinates in top-left, top-right,\n",
    "\t# bottom-right, and bottom-left order\n",
    "\treturn np.array([tl, tr, br, bl], dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_4_corners(img, template, method):\n",
    "\t\"\"\"Finds the coordiantes of the corners of the map; \t\n",
    "\targuments :\t\n",
    "\t| img : image in the bgr color space;\t\n",
    "\t| template : image that has been filtered and is in the binary color space;\t\n",
    "\t| method : cross-correlation method for template matching\n",
    "\t\"\"\"\n",
    "\tis_found = False\n",
    "\t\n",
    "\t#filter the image\n",
    "\tfiltered_img = cv2.GaussianBlur(img,(5,5), 1)\n",
    "\t\n",
    "\t#convert image and template to binary\n",
    "\timage_binary = filter_grayscale(filtered_img)\n",
    "\ttemplate_binary = filter_grayscale(template)\n",
    "\t\n",
    "\t#cv2.imshow(\"binary of black\", image_binary)\n",
    "\t#get template dimensions\n",
    "\tc, w, h  = template.shape[::-1]\n",
    "\t#print(template.shape[::-1])\n",
    "\t\n",
    "\t#match template to image\n",
    "\tres = cv2.matchTemplate(image_binary,template_binary,method)\n",
    "\t(yCoords, xCoords) = np.where(res >= RES_THRESHOLD_BLACK)\n",
    "\n",
    "\n",
    "\t# initialize our list of rectangles\n",
    "\trects = []\n",
    "\t# loop over the starting (x, y)-coordinates again\n",
    "\tfor (x, y) in zip(xCoords, yCoords):\n",
    "\t\t# update our list of rectangles\n",
    "\t\trects.append((x, y, x + w, y + h))\n",
    "\t\n",
    "\t# apply non-maxima suppression to the rectangles\n",
    "\tpick = non_max_suppression_fast(np.array(rects), OVERLAP_THRESHOLD)\n",
    "\t#print(\"[INFO] {} matched locations after NMS\".format(len(pick)))\n",
    "\t\n",
    "\t#create a list of centers of the rectangles\n",
    "\tcross_points = []\n",
    "\tfor (startX, startY, endX, endY) in pick:\n",
    "\t    # draw the bounding box on the image\n",
    "\t\tcv2.rectangle(image_binary, (startX, startY), (endX, endY), (0, 0, 255), 3)\n",
    "\t\tcross_points.append([int((startX+endX)/2), int((startY+ endY)/2)]) \n",
    "\n",
    "\t#cv2.imshow(\"binary of black\", image_binary)\n",
    "\t#cv2.imshow(\"template\", template_binary)\n",
    "\n",
    "\tif (len(pick) == 4) :\n",
    "\t\tis_found = True\n",
    "\t\n",
    "\tcross_points = np.array(cross_points)\n",
    "\tcv2.imwrite(\"template_cross_binary.png\", image_binary)\n",
    "\t\n",
    "\treturn is_found, cross_points\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warp the camera Image\n",
    "=====================\n",
    "\n",
    "This step converts the camera image to a bird's eye view of the map, where we are right on top of the map, and the map fills the view entirely.\n",
    "To do this, we detect the 4 crosses in the corners of the map.\\\n",
    "To detect the crosses, we first filter the camera image, then convert it to grayscale, then to binary to get an image where only the crosses are black and the rest is white.\\\n",
    "We then apply match template to this binary image to find all the crosses, using the method 'cv2.TM_CCORR_NORMED', as it is the one that gives the highest correlation score.\\\n",
    "The problem with matchtemplate is that it can find multiple times the same cross. If there is a high correlation between the template and a part of the image at one position, if the template shifts by one pixel on the image, the correlation between the image and the template at this new position will still be very high. This means that matchtemplate alone doesn't give only the positions of the four crosses.\\\n",
    "To get only four positions, we create rectangles at each location where the cross correlation of match template is above a threshold. We then compute the intersection between all of the rectangles and keep the rectangles with the highest cross correlation until no rectangles intersect each other. This guarantees in practice that the four corners are separated.\\\n",
    "We then order the corners, and warp the image thanks to a perspective transform where we force the corners of the map to bbe in the corners of the new image created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"report/warp_image.png\">\n",
    "<img src=\"report/bird_image.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warp_image(image, template, method, old_corners):\n",
    "\t\"\"\"Transforms the camera image to a map bird's eye view with only the map.\n",
    "\tArguments:\n",
    "\t\timage: image of the camera in the bgr colorspace.\n",
    "\t\ttemplate: template of the cross.\n",
    "\t\tmethod: cross-correlation method.\n",
    "\t\told_corners: corners found previously by the function.\n",
    "\tReturns:\n",
    "\t\tis_found: boolean true if corners have been found.\n",
    "\t\tbird_image: projected image of the camera.\n",
    "\t\tmethod: cross correlation method for matchTemplate.\n",
    "\t\told_corners: corners found previously.\"\"\"\n",
    "\tis_found, cross_points = find_4_corners(image, template, method)\n",
    "\t\n",
    "\tif is_found:\n",
    "\t\tordered_crosses = order_points(cross_points)\n",
    "\t\t#store corners positions in case we have a frame where we don't see them anymore\n",
    "\t\told_corners = ordered_crosses\n",
    "\telse : \n",
    "\t\t#will become the last found corners, \n",
    "\t\tordered_crosses = old_corners\n",
    "\n",
    "\tordered_crosses = np.float32(ordered_crosses)\n",
    "\tbird_corners = np.float32(BIRD_CORNERS)\n",
    "\n",
    "\tmatrix = cv2.getPerspectiveTransform(ordered_crosses, bird_corners)\n",
    "\tbird_image = cv2.warpPerspective(image, matrix, (BIRD_WIDTH, BIRD_HEIGHT))\n",
    "\n",
    "\treturn is_found, bird_image, old_corners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detect the obstacles\n",
    "====================\n",
    "\n",
    "To detect the obstacles, and filter the colors in general, we convert the image to the hsv colorspace. In the hsv colorspace, the colors are represented with the hue, which in opencv is an angle between 0 and 180 degrees. This makes it easier to select the range corresponding to a color in particular.\\\n",
    "In our map, the obstacles are blue, which correspond to a hue angle around 100 degrees. We do a band pass filter where we keep the pixels with an angle close to that of the blue. \\\n",
    "We then keep the value part of the hsv, which corresponds to the grayscale, then apply a binary threshold so that the white part in the binary image are the obstacles.\\\n",
    "Because the thymio is bigger than one pixel, and because we use the A* algorithm to generate the global path, we dilate the obstacles by half the width of the thymio. That way, we can approximate the thymio as one pixel wide.\\\n",
    "We then also create obstacles in the borders to ensure that the thymio doesn't get out of the map.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"report/detect_obstacles.png\">\n",
    "<img src = \"report/map_in_polydome2.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_obstacles(hsv_image):\n",
    "\t\"\"\"Finds the obstacles (blue parts) on the map and put them on the map\"\"\"\n",
    "\t#filter the orange color\n",
    "\t# hsv_min, hsv_max = band_pass_hsv(BLUE_BGR) # should not exist\n",
    "\tmask = cv2.inRange(hsv_image, HSV_BLUE_MIN, HSV_BLUE_MAX)\n",
    "\toutput = cv2.bitwise_and(hsv_image,hsv_image, mask= mask)\n",
    "\t#cv2.imshow(\"output of blue filter\", output)\n",
    "\t#convert map to binary (0 = no obstacle, 1 = obstacle)\n",
    "\toutput_gray = output[:, :, 2]\n",
    "\t\n",
    "\tthreshold = BINARY_THRESHOLD_BLUE\n",
    "\tret, map = cv2.threshold(output_gray, threshold, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "\t#map[map==255] = OBSTACLE_MAP_VALUE\n",
    "\treturn map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dilate_obstacles(map, width):\n",
    "\t\"\"\"Dilates the obstacles to approximate the size of the thymio to one pixel.\n",
    "\tArguments:\n",
    "\t\tmap: image of 0 (background) and 1 (obstacles).\n",
    "\t\twidth: width of dilation of the obstacles.\n",
    "\tReturns:\n",
    "\t\tdilated_img: image with dilated obstacles.\"\"\"\n",
    "\n",
    "\tdilation_shape = cv2.MORPH_RECT\n",
    "\tkernel = cv2.getStructuringElement(dilation_shape, (2*width + 1, 2*width + 1), (width, width))\n",
    "\tkernel_noise = cv2.getStructuringElement(dilation_shape, (2*NOISE_WIDTH + 1, 2*NOISE_WIDTH + 1), (NOISE_WIDTH, NOISE_WIDTH))\n",
    "\teroded_image = cv2.erode(map, kernel_noise, iterations = 1)\n",
    "\teroded_image = cv2.dilate(eroded_image, kernel_noise, iterations = 1)\n",
    "\tdilated_img = cv2.dilate(eroded_image, kernel, iterations = 1)\n",
    "\n",
    "\treturn dilated_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_map_borders(map, width):\n",
    "\t\"\"\"put the borders of the map as obstacles\"\"\"\n",
    "\tmap[0:width, :MAP_WIDTH] = 255\n",
    "\tmap[:MAP_HEIGHT, MAP_WIDTH-width:MAP_WIDTH] = 255\n",
    "\tmap[MAP_HEIGHT-width: MAP_HEIGHT, :MAP_WIDTH] = 255\n",
    "\tmap[:MAP_HEIGHT, 0:width] = 255\n",
    "\treturn map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_map(hsv_image):\n",
    "\t\"\"\"take the bird's eye image and convert it to a map, without the goal coordinates.\n",
    "\tArguments:\n",
    "\t\thsv_image: image in the hsv colorspace.\n",
    "\tReturns:\n",
    "\t\tmap in the binary colorspace.\"\"\"\n",
    "\t#projects map to smaller array\n",
    "\tbird_corners = np.array(BIRD_CORNERS, dtype = np.float32)\n",
    "\tmap_corners = np.array(MAP_CORNERS, dtype = np.float32)\n",
    "\tmatrix = cv2.getPerspectiveTransform(bird_corners, map_corners)\n",
    "\tbird_image = cv2.warpPerspective(hsv_image, matrix, (MAP_WIDTH, MAP_HEIGHT))\n",
    "\t\n",
    "\tobstacles = create_obstacles(bird_image)\n",
    "\t\n",
    "\tdilated_map = dilate_obstacles(obstacles, THYMIO_WIDTH)\n",
    "\tcv2.imwrite(\"dilated_map_before_borders.jpg\", dilated_map)\n",
    "\tmap = create_map_borders(dilated_map, THYMIO_WIDTH)\n",
    "\t#map[map == 255] = OBSTACLE_MAP_VALUE\n",
    "\treturn map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detect the goal\n",
    "===============\n",
    "Just as with the obstacles, I convert the bird's eye view of the map to hsv color and then apply the bandpass filter to only keep the green. I then convert this filtered image to binary to only what was previously green as white, the rest in black. This results to a black picture with a white circle inside.\\\n",
    "\n",
    "I then use the function findContours to find the contours of the circle. This functions applies some sort of edge detection and then links the contours together to create an object. We order all of the contours found by their area, and keep the contour with the biggest area. \\\n",
    "We then convert this contour to a moment. This effectively gets all of the points that are within the area formed by the contour. As each point gets assigned a position and a weight, we can compute the centroid of the contour. If the contour has a big enough area, then the goal position is found, and the position is the centroid of the moment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"report/detect_goal.png\">\n",
    "<img src = \"report/green_filter.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_goal(hsv_img):\n",
    "\t\"\"\"Finds coordinates of goal.\n",
    "\tArguments:\n",
    "\t\tbw: image in the binary color space.\t\n",
    "\t\ttemplate: image that has been filtered and is in the binary color space (1 dim).\n",
    "\t\tmethod: cross-correlation method for template matching.\n",
    "\tReturns:\n",
    "\t\tis_found: boolean true if goal has been found.\n",
    "\t\tposition: position (y, x) in array coordinates of the goal.\n",
    "\t\t\n",
    "\t\"\"\"\n",
    "\tis_found = False\n",
    "\tmask = cv2.inRange(hsv_img, HSV_GREEN_MIN, HSV_GREEN_MAX)\n",
    "\toutput = cv2.bitwise_and(hsv_img,hsv_img, mask= mask)\n",
    "\t#cv2.imshow(\"green banddpass\", output)\n",
    "\tgrayscale = output[:, :, 2]\n",
    "\t#convert to binary\n",
    "\tthreshold = BINARY_THRESHOLD_GREEN\n",
    "\tret, bw = cv2.threshold(grayscale, threshold, 255, cv2.THRESH_BINARY)\n",
    "\t\t\n",
    "\t#cv2.imshow(\"green filter\", bw)\n",
    "\t#cv2.imwrite(\"red_filter.jpg\", bw)\n",
    "\t\n",
    "\t#find location of thymio\n",
    "\tcontours, hierarchy  = cv2.findContours(bw, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "\t#print(len(contours), \"elements found\")\n",
    "\tif len(contours) != 0:\n",
    "\t\t#if contour detected, keep the biggest one (most likely the thymio)\n",
    "\t\tcontours = sorted(contours, key = cv2.contourArea, reverse = True)\n",
    "\t\tcontour = contours[0]\n",
    "\n",
    "\t\tcontour_threshold = 30 #detect if contour found is too small\n",
    "\t\tM = cv2.moments(contour)\n",
    "\t\tif (M[\"m00\"] != 0) and (cv2.contourArea(contour) >= contour_threshold) :\n",
    "\t\t\tcX = int(M[\"m10\"]/M[\"m00\"])\n",
    "\t\t\tcY = int(M[\"m01\"]/M[\"m00\"])\n",
    "\t\t\tis_found = True\n",
    "\t\t\tposition = (cY//2, cX//2)\n",
    "\t\telse :\n",
    "\t\t\t#division by zero or no contour big enough found\n",
    "\t\t\t#print(M[\"m00\"], \"contour too small\") \n",
    "\t\t\tposition = (0, 0)\n",
    "\t\t\tis_found = False\n",
    "\telse:\n",
    "\t\t#no contour found\n",
    "\t\t#print(\"no contours found\")\n",
    "\t\tposition = (0, 0)\n",
    "\t\tis_found = False\n",
    "\t\n",
    "\treturn is_found, position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detect the thymio\n",
    "=================\n",
    "Like before, I use a bandpass filter on the bird's eye view of the map that has been converted to the hsv colorspace, this time to filter the color red.\\\n",
    "On the thymio, there are two circles, one bigger than the other. \\\n",
    "Using findContours, I keep the two biggest circles above an area threshold (to not compute the position of noise) and compute their centroid. \\\n",
    "Using the two positions, I can extract the position of the thymio, which is the center between the centroid, and the angle of the thymio, by using the arctan of the differences in x and y. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"report/detect_thymio.png\">\n",
    "<img src = \"report/red_filter.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_l_thymio(bw):\n",
    "\t\"\"\"Finds coordinates of thymio.\n",
    "\tArguments:\n",
    "\t\tbw: image in the binary colorspace.\n",
    "\tReturns:\n",
    "\t\tis_found: boolean true if thymio has been found.\n",
    "\t\tposition: position (x, y) with x, y distances from top left corner\n",
    "\t\tangle: angle of the thymio in radians in [-pi, pi].\n",
    "\t\"\"\"\n",
    "\tis_found = False\n",
    "\n",
    "\t#find location of thymio\n",
    "\tcontours, hierarchy  = cv2.findContours(bw, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "\t#print(len(contours), \"elements found\")\n",
    "\tif len(contours) >= 2:\n",
    "\t\t#if contour detected, keep the biggest one (most likely the thymio)\n",
    "\t\tcontours = sorted(contours, key = cv2.contourArea, reverse = True)\n",
    "\t\tbig_circle = contours[0]\n",
    "\t\tsmall_circle = contours[1]\n",
    "\n",
    "\t\tcontour_threshold = 10 #detect if contour found is too small\n",
    "\t\tM_big = cv2.moments(big_circle)\n",
    "\t\tM_small = cv2.moments(small_circle)\n",
    "\t\tif (M_big[\"m00\"] != 0) and (M_small[\"m00\"] != 0) and (cv2.contourArea(big_circle) >= contour_threshold) and (cv2.contourArea(small_circle) >= contour_threshold):\n",
    "\t\t\tcX_big = int(M_big[\"m10\"]/M_big[\"m00\"])\n",
    "\t\t\tcY_big = int(M_big[\"m01\"]/M_big[\"m00\"])\n",
    "\t\t\tcX_small = int(M_small[\"m10\"]/M_small[\"m00\"])\n",
    "\t\t\tcY_small = int(M_small[\"m01\"]/M_small[\"m00\"])\n",
    "\t\t\tis_found = True\n",
    "\t\t\tposition = ((cX_big + cX_small)//4, (cY_big + cY_small)//4)\n",
    "\t\t\tdelta_x = cX_big - cX_small\n",
    "\t\t\tdelta_y = cY_big - cY_small\n",
    "\t\t\tangle = -(np.arctan2(delta_y, delta_x))\n",
    "\t\telse :\n",
    "\t\t\t#division by zero or no contour big enough found\n",
    "\t\t\t#print(M[\"m00\"], \"contour too small\") \n",
    "\t\t\tposition = (0, 0)\n",
    "\t\t\tis_found = False\n",
    "\t\t\tangle = 0\n",
    "\telse:\n",
    "\t\t#no contour found\n",
    "\t\t#print(\"no contours found\")\n",
    "\t\tposition = (0, 0)\n",
    "\t\tis_found = False\n",
    "\t\tangle = 0\n",
    "\t\t\n",
    "\treturn is_found, position, angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_camera_obstructed(image):\n",
    "    \"\"\"Given image return boolean true if the camera is obstructed\"\"\"\n",
    "    img_grayscale = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\t#convert to binary both image and template\n",
    "    threshold = 150\n",
    "    \n",
    "    ret, image_binary = cv2.threshold(img_grayscale, threshold, 255, cv2.THRESH_BINARY)\n",
    "    zero_points = cv2.findNonZero(image_binary)\n",
    "    \n",
    "    if (zero_points is None) or (len(zero_points) < 200):\n",
    "        is_obstructed = True\n",
    "    else:\n",
    "        is_obstructed = False\n",
    "    return is_obstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_image(image, old_corners):\n",
    "    ordered_crosses = np.float32(old_corners)\n",
    "    bird_corners = np.float32(BIRD_CORNERS)\n",
    "    matrix = cv2.getPerspectiveTransform(ordered_crosses, bird_corners)\n",
    "    bird_image = cv2.warpPerspective(image, matrix, (BIRD_WIDTH, BIRD_HEIGHT))\n",
    "\n",
    "    return bird_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_thymio_pose(bird_image):\n",
    "\t\"\"\"returns position and angle of thymio.\n",
    "\tArguments:\n",
    "\t\timage: bird's eye image in the bgr colorspace.\n",
    "\tReturns:\n",
    "\t\tis_found: boolean true if thymio has been found.\n",
    "\t\tposition: position (y, x) with x, y distances from top left corner.\n",
    "\t\tangle: angle of the thymio in radians in [-pi, pi].\"\"\"\n",
    "\thsv_img = filter_to_hsv(bird_image)\n",
    "\t#keep only red and filter to binary\n",
    "\tbw = filter_hsv_to_binary(hsv_img, RED_BGR, BINARY_THRESHOLD_RED) \n",
    "\n",
    "\tcv2.imwrite(\"hsv_image_fed_to_red_filter.jpg\", bw) #for debug, to get rid of?\n",
    "\tcv2.imwrite(\"bird_image.jpg\", bird_image)\n",
    "\t#find position of thymio\n",
    "\tis_found, position, angle = find_l_thymio(bw)\n",
    "\t\t\n",
    "\tposition = position[::-1]\n",
    "\t# print(\"position_thymio\", is_found, position, angle)\n",
    "\t# print('............................................')\n",
    "\t\n",
    "\treturn is_found, position, angle #for now, the position is a pixel location, will need to convert it later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_setup(cam):\n",
    "\t\"\"\"Initialise the vision, waits for the camera to calibrate and give essential informations.\n",
    "\tArguments:\n",
    "\t\tcam: camera opencv object.\n",
    "\tReturns:\n",
    "\t\tmap: array where background is zero and obstacles are -1.\n",
    "\t\tgoal_position: position (y, x) of the goal in array coordinates.\n",
    "\t\tthymio_position: position (y, x) of the thymio in array coordinates.\n",
    "\t\tangle: angle of the thymio in radians between [-pi, pi].\n",
    "\t\told_corners: last positions of the four corners of the map in the camera image.\"\"\"\n",
    "\t\n",
    "\t#set it for later\n",
    "\told_corners = CAMERA_CORNERS #variable that will be updated each time\n",
    "\tmap = np.zeros((BIRD_HEIGHT, BIRD_WIDTH))\n",
    "\tbird_image = np.zeros((BIRD_HEIGHT, BIRD_WIDTH))\n",
    "\thsv_img = np.zeros((BIRD_HEIGHT, BIRD_WIDTH, 3))\n",
    "\ti = 0\n",
    "\twhile i !=30:\n",
    "\t\ti +=1\n",
    "\t\tret, image = cam.read()\n",
    "\t\t\n",
    "\t\tis_thymio_found = False\n",
    "\t\tcorners_found, bird_image, old_corners = warp_image(image, template_cross, method, old_corners)\n",
    "\t\tbird_image_copy = bird_image.copy() #picture\n",
    "\t\t\n",
    "\t\tprint(\"corners found\", corners_found)\n",
    "\t\thsv_img = filter_to_hsv(bird_image)\n",
    "\t\tmap = create_map(hsv_img)\n",
    "\t\tcv2.imwrite(\"map_after_createMap.jpg\", map)\n",
    "\t\tis_goal_found, goal_position = find_goal(hsv_img)\n",
    "\t\t\n",
    "\t\tis_thymio_found, thymio_position, angle = get_thymio_pose(bird_image)\n",
    "\t\t\n",
    "\t\t\n",
    "\n",
    "\t\tcv2.imshow(\"Bird's eye view\", bird_image)\n",
    "\n",
    "\t# Release the capture and writer objects\n",
    "\t\n",
    "\tcv2.destroyAllWindows()\n",
    "\tmap = np.float32(map)\n",
    "\tmap[map == 255] = -1\n",
    "\t\n",
    "\n",
    "\treturn map, goal_position, thymio_position, angle, old_corners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_corners = CAMERA_CORNERS #variable that will be updated each time\n",
    "\n",
    "is_cam_setup = False\n",
    "is_cam_obstructed = False\n",
    "map = np.zeros((BIRD_HEIGHT, BIRD_WIDTH))\n",
    "bird_image = np.zeros((BIRD_HEIGHT, BIRD_WIDTH))\n",
    "hsv_img = np.zeros((BIRD_HEIGHT, BIRD_WIDTH, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MatchTemplate vs FindContours\n",
    "=============================\n",
    "\n",
    "For the detection of the corners of the map, I use the opencv function Matchtemplate. In this case, Matchtemplate works well because the angle of the crosses doesn't change, and it can detect multiple elements in the picture, whilst still being quite robust. Because the corners are black and not white, findcontours would not work in this configuration, and it would be less robust if there is a shade of dark around the map for instance.\\\n",
    "For the goal, I could have used matchtemplate as well, but FindContour proved to be robust and faster, so I chose to use it.\\\n",
    "For the thymio position, I had a problem with matchtemplate when the thymio was too close to the borders, because then the template would be limited by the limits of the image and the given position and angle wouldn't be as precise, whereas FindContours was still robust in those cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path Planning Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tried a few different path planning methods, like Dijkstra’s algorithm and others. While Dijkstra works fine, it’s slower because it doesn’t use any kind of heuristic to guide the search, so it ends up exploring more paths than necessary. After testing a few options, we found that A*, using an 8-connected grid, was the best choice. It allows movement in all 8 directions (including diagonals), which gives it more flexibility and helps avoid unnecessary exploration. Overall, A* was faster and more efficient than the other methods, so we decided to go with it for our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heuristic(a, b):\n",
    "    \"\"\"\n",
    "    Calculate the heuristic cost between 2 points, which specifically the current position and the goal.\n",
    "    \n",
    "    Parameters:\n",
    "    a (tuple): The input position.\n",
    "    b (tuple): The input goal.\n",
    "    \n",
    "    Return:\n",
    "    float: The output heuristic value between the current position and the goal.\n",
    "    \"\"\"\n",
    "    \n",
    "    return abs(a[0] - b[0]) + abs(a[1] - b[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_planning_a_star(map_grid, start, goal):\n",
    "    \"\"\"\n",
    "    Calculate the optimal map to travel from the start position to the goal position using A* algorithm with Euclidean distance (8-connected grid).\n",
    "    The function is inspired by the A* implementation with 4-connected grid from the MICRO-452: Basics of Mobile Robotics by Professor Francesco Mondada.\n",
    "    \n",
    "    Parameters:\n",
    "    map_grid (matrix): The input map with value 0 for unoccupied cells and -1 for occupied cells (obstacles).\n",
    "    start (tuple): The input start position.\n",
    "    goal (tuple): The input goal position.\n",
    "    \n",
    "    Return:\n",
    "    list: The output path as a list of tuples which are sub-goals to be covered to travel from the start position to the goal position.    \n",
    "    \"\"\"\n",
    "    \n",
    "    def lowest_f_costs(open_list, f_costs):\n",
    "        # identify the cell in open_list wwith the lowest f_cost\n",
    "        lowest_idx = 0\n",
    "        for i in range(len(open_list)):\n",
    "            if f_costs[open_list[i]] < f_costs[open_list[lowest_idx]]:\n",
    "                lowest_idx = i\n",
    "        return lowest_idx\n",
    "        \n",
    "    # initialization\n",
    "    open_list = [start] # track unexplored cells\n",
    "    explored = [] # track explored cells\n",
    "    \n",
    "    came_from = {} # track parent cell of each cells\n",
    "    g_costs = {start: 0} # g_cost at each cells\n",
    "    f_costs = {start: heuristic(start, goal)} # f_cost at each cells = g_cost + h_cost (heuristic)\n",
    "    \n",
    "    while open_list:\n",
    "        # pop the cell with the lowest f_cost from the open set\n",
    "        current_idx = lowest_f_costs(open_list, f_costs)\n",
    "        current_pos = open_list.pop(current_idx)\n",
    "        explored.append(current_pos)\n",
    "\n",
    "        # reconstruct path\n",
    "        if current_pos == goal:\n",
    "            break\n",
    "        \n",
    "        # get the neighbors of the current cell\n",
    "        neighbors = [\n",
    "            (current_pos[0] - 1, current_pos[1]),     # up\n",
    "            (current_pos[0] + 1, current_pos[1]),     # down\n",
    "            (current_pos[0], current_pos[1] - 1),     # left\n",
    "            (current_pos[0], current_pos[1] + 1),     # right\n",
    "            \n",
    "            (current_pos[0] - 1, current_pos[1] - 1), # top-left\n",
    "            (current_pos[0] - 1, current_pos[1] + 1), # top-right\n",
    "            (current_pos[0] + 1, current_pos[1] - 1), # bottom-left\n",
    "            (current_pos[0] + 1, current_pos[1] + 1)  # bottom-right\n",
    "        ]\n",
    "\n",
    "        for neighbor in neighbors:\n",
    "            # check if neighbor is within bounds and not an obstacle\n",
    "            if (0 <= neighbor[0] < map_grid.shape[0]) and (0 <= neighbor[1] < map_grid.shape[1]):\n",
    "                if map_grid[neighbor[0], neighbor[1]] != -1 and neighbor not in explored:\n",
    "                    # calculate g_costs_updated, considering diagonal steps and probably add cost to g_scores if cells are near obstacles\n",
    "                    if (neighbor[0] != current_pos[0]) and (neighbor[1] != current_pos[1]):\n",
    "                        g_costs_updated = g_costs[current_pos] + np.sqrt(2)  # diagonal move cost\n",
    "                    else:\n",
    "                        g_costs_updated = g_costs[current_pos] + 1  # adjacent move cost\n",
    "\n",
    "                    # record the best path\n",
    "                    if neighbor not in g_costs or g_costs_updated < g_costs[neighbor]:\n",
    "                        came_from[neighbor] = current_pos\n",
    "                        g_costs[neighbor] = g_costs_updated\n",
    "                        f_costs[neighbor] = g_costs_updated + heuristic(neighbor, goal)\n",
    "\n",
    "                        if neighbor not in open_list:\n",
    "                            open_list.append(neighbor)\n",
    "\n",
    "    # reconstruct path\n",
    "    if current_pos == goal:\n",
    "        path = []\n",
    "        while current_pos in came_from:\n",
    "            path.append(current_pos)\n",
    "            current_pos = came_from[current_pos]\n",
    "        path.append(start)\n",
    "        path.reverse()\n",
    "        return path\n",
    "    else:\n",
    "        # no path was found\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thymio_transform_path(path, start):\n",
    "    \"\"\"\n",
    "    Transform the path from vision reference (y-axis right-sided, x-axis downward) to\n",
    "    new reference with origin at the start position (y-axis upward, x-axis right-sided).\n",
    "    \n",
    "    Parameters: \n",
    "    path (list): The input path as a list of tuples which are sub-goals to be covered to travel from the start position to the goal position. \n",
    "    start (tuple): The input start position.\n",
    "    \n",
    "    Return:\n",
    "    list: The ouput path as a list of tuples (sub-goals) in the new reference at the start position.\n",
    "    \"\"\"\n",
    "    \n",
    "    start_x, start_y = start\n",
    " \n",
    "    path_transformed = []\n",
    "\n",
    "    # transform the reference\n",
    "    for x, y in path:\n",
    "        new_x = (y - start_y)\n",
    "        new_y = -(x - start_x)\n",
    "        \n",
    "        path_transformed.append((new_x, new_y))\n",
    "        \n",
    "    return path_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_combining(path):\n",
    "    \"\"\"\n",
    "    Add the orientation toward the subsequent sub-goal for each sub-goal in the path, which will be used to simplify the path later.\n",
    "    \n",
    "    Parameters:\n",
    "    path (list): The input path as a list of tuples which are sub-goals to be covered to travel from the start position to the goal position.\n",
    "    \n",
    "    Return:\n",
    "    list: The ouput path as a list of tuples (sub-goals) with orientation toward the subsequent sub-goal.\n",
    "    \"\"\"\n",
    "    \n",
    "    path_translate = path\n",
    "    path_rotate = []\n",
    "    path_combined = []\n",
    "    \n",
    "    # check if the map has at least 2 points\n",
    "    if len(path) < 2:\n",
    "        return [] \n",
    "    \n",
    "    # calculate robot's orientation at each position toward the subsequent sub-goal\n",
    "    for i in range(len(path_translate) - 1):\n",
    "        dx = path_translate[i+1][0] - path_translate[i][0]\n",
    "        dy = path_translate[i+1][1] - path_translate[i][1]\n",
    "        \n",
    "        angle = np.arctan2(dy, dx)\n",
    "        \n",
    "        path_rotate.append(angle)\n",
    "    \n",
    "    # generate final path with both positions and orientation\n",
    "    for i in range(len(path_translate) - 1):\n",
    "        x_coordinate = path_translate[i][0]\n",
    "        y_coordinate = path_translate[i][1]\n",
    "        angle = path_rotate[i]\n",
    "        \n",
    "        path_combined.append((x_coordinate, y_coordinate, angle))\n",
    "    \n",
    "    if path:\n",
    "        last_point = path[-1]\n",
    "        last_angle = path_rotate[-1] if path_rotate else 0  # default to 0 if no angles calculated\n",
    "        path_combined.append((last_point[0], last_point[1], last_angle))\n",
    "    \n",
    "    return path_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_finalizing(path, start, scale_factor = SCALING_FACTOR):\n",
    "    \"\"\"\n",
    "    Simplify the path by reducing the sub-goals with the same orientation toward the subsequent sub-goals\n",
    "    and convert to the real world scale.\n",
    "    \n",
    "    Parameters:\n",
    "    path (list): The input path as a list of tuples (sub-goals) with orientation toward the subsequent sub-goal.\n",
    "    start (tuple): The input start position.\n",
    "    scale_factor (float): The input scaler to convert from matrix world (in pixels) to real world (in milimeters).\n",
    "    \n",
    "    Return:\n",
    "    list: The output simplified path as a list of tuples (critical sub-goals) and converted to real world in milimeter scale.\n",
    "    \"\"\"\n",
    "    \n",
    "    path = thymio_transform_path(path, start)\n",
    "    path = path_combining(path)\n",
    "    \n",
    "    path_final = [path[0]]\n",
    "    \n",
    "    # simplify the path into critical sub-goals by reducing ones with the same orientation\n",
    "    for pose in path[1:]:\n",
    "        if pose[2] != path_final[-1][2]:\n",
    "            path_final.append(pose)\n",
    "    \n",
    "    # add goal position to the list\n",
    "    path_final.append(path[-1])\n",
    "\n",
    "    # convert the matrix world (in pixels) to real world (in milimeters)\n",
    "    path = [(x * scale_factor, y * scale_factor, z) for x, y, z in path]     \n",
    "        \n",
    "    return path_final # (x, y, theta_rad)_t_reduced_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Avoidance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_neural = np.array([0,0])\n",
    "sensor = np.array([0,0,0,0,0,0,0])\n",
    "is_obstacle = False\n",
    "motor_target = [0,0]\n",
    "\n",
    "def local_avoidance(prox_horizontal):\n",
    "    global  motor_target, is_obstacle\n",
    "\n",
    "    #Weights\n",
    "    w_l = [10, 3 , 2, 1, 1,  1, 1]\n",
    "    w_r = [1, 1, 10,  3,  5, 1,  1]\n",
    "    \n",
    "\n",
    "    # Scale factors for sensors and constant factor\n",
    "    sensor_scale = 500\n",
    "    sensor = prox_horizontal\n",
    "    \n",
    "    for i in range(5):\n",
    "        \n",
    "        if sensor[0] == sensor[1] == sensor[2] == sensor[3] == sensor[4] == 0:\n",
    "            output_neural[0] = output_neural[1] = 0\n",
    "            is_obstacle = False\n",
    "        else:\n",
    "            is_obstacle = True\n",
    "            \n",
    "        # Get and scale inputs\n",
    "        sensor[i] = prox_horizontal[i] // sensor_scale\n",
    "        \n",
    "        # Compute outputs of neurons and set motor powers\n",
    "        output_neural[0] = output_neural[0] + sensor[i] * w_l[i] #left motor\n",
    "        output_neural[1] = output_neural[1] + sensor[i] * w_r[i] #right motor\n",
    "            \n",
    "    \n",
    "    # Set motor powers\n",
    "    motor_target [0] = output_neural[0] # motor left\n",
    "    motor_target [1] = output_neural[1] # motor right\n",
    "    \n",
    "    return motor_target, is_obstacle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kidnapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "kidnapped = False\n",
    "\n",
    "def kidnapping(acceleration, sensor):\n",
    "   global kidnapped\n",
    "\n",
    "   if (abs(acceleration[0]) > KIDNAPPING_THRESHOLD or abs(acceleration[1]) > KIDNAPPING_THRESHOLD) and (sensor[5] != 0 or sensor[6] != 0):\n",
    "      kidnapped = True\n",
    "      # print('kidnapped: ', kidnapped)\n",
    "   else:\n",
    "      kidnapped = False\n",
    "      # print('kidnapped: ', kidnapped)\n",
    "   \n",
    "   return kidnapped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The state variable for the Kalman filter are \n",
    "\n",
    "\\begin{cases}\n",
    "   x_{+} = x_{0} + v*\\Delta_{t}*cos(\\theta)\\\\\n",
    "   y_{+} = y_{0} + v*\\Delta_{t}*sin(\\theta)\\\\\n",
    "   \\theta_{+} = \\theta_{0} + \\phi\\\\\n",
    "   v_{right}\\\\\n",
    "   v_{left}\n",
    "\\end{cases}\n",
    "\n",
    "This allow us to first calculate the Jacobian matrix. Furthermore, we distinguish between two cases: the first case occurs when the camera is obscured, in which only the left and right speed states are accessible. The second case applies when the camera is not hidden, and all state variables are utilized.\n",
    "\n",
    "To determine the components of the process noise and measurement noise matrices, we conducted several experiments with the Thymio. For example, we instructed it to travel a certain distance for 10 seconds before stopping. We then measured the distance it covered. By repeating this experiment multiple times, we obtained a dataset with varying positions, from which we extracted  the variance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_est= y_est =  0 \n",
    "is_initialized = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def odometrie(x_prev,y_prev,theta_prev,motor_r,motor_l):\n",
    "   \n",
    "    v = (motor_r + motor_l)*THYMIO_MMS/2\n",
    "    w = (motor_r - motor_l)*THYMIO_MMS\n",
    "\n",
    "    theta = theta_prev  + w*DELTA_T/D_BASELINE\n",
    "    \n",
    "    x = x_prev + v*DELTA_T*np.cos(theta_prev)\n",
    "    y = y_prev + v*DELTA_T*np.sin(theta_prev)\n",
    "\n",
    "    return x,y, theta , motor_r, motor_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Jacobien(theta, v_right, v_left): \n",
    "    \"\"\"\n",
    "    Computes the Jacobian matrix for the Kalman filter, \n",
    "    \"\"\"\n",
    "    alpha = THYMIO_MMS * (v_right + v_left) / 2\n",
    "    beta = THYMIO_MMS * (v_right - v_left) / D_BASELINE\n",
    "\n",
    "    A = - (alpha * DELTA_T * np.sin(theta + DELTA_T * beta))\n",
    "    B = (1/2) * DELTA_T * np.cos(theta + DELTA_T * beta) - (alpha * (1 / D_BASELINE) * DELTA_T**2 * np.sin(theta + DELTA_T * beta))\n",
    "    C = (1/2) * DELTA_T * np.cos(theta + DELTA_T * beta) + (alpha * (1 / D_BASELINE) * DELTA_T**2 * np.cos(theta + DELTA_T * beta))\n",
    "    D = alpha * DELTA_T * np.cos(theta + DELTA_T * beta)\n",
    "    E = (1/2) * DELTA_T * np.sin(theta + DELTA_T * beta) + (alpha * (1 / D_BASELINE) * DELTA_T**2 * np.cos(theta + DELTA_T * beta))\n",
    "    F = (1/2) * DELTA_T * np.sin(theta + DELTA_T * beta) - (alpha * (1 / D_BASELINE) * DELTA_T**2 * np.cos(theta + DELTA_T * beta))\n",
    "    G = DELTA_T/D_BASELINE\n",
    "    H = -DELTA_T/D_BASELINE\n",
    "    \n",
    "    J = np.array([[1, 0, A, B, C],\n",
    "                  [0, 1, D, E, F],\n",
    "                  [0, 0, 1, G, H],\n",
    "                  [0, 0, 0, 1, 0],\n",
    "                  [0, 0, 0, 0, 1]])\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should not exist\n",
    "# q_x = q_y = 2.74e-1\n",
    "# q_theta = 1.24e-1\n",
    "# q_vx = q_vy = 1.52\n",
    "# x_odom = y_odom = 0\n",
    "\n",
    "q_x = q_y = 0.00017\n",
    "q_theta = 0.00098\n",
    "q_vx = q_vy = 0.0178\n",
    "\n",
    "def EKF(x, y, theta, v_r, v_l, mu_pred_prev, u_state_prev, sigma_pred_prev, R ,C, not_vision):\n",
    "    \n",
    "    A = np.array([[1, 0, 0, 0, 0],\n",
    "             [0, 1, 0, 0, 0],\n",
    "             [0, 0, 1, 0, 0],\n",
    "             [0, 0, 0, 0, 0],\n",
    "             [0, 0, 0, 0, 0]])\n",
    "    \n",
    "    Q = np.array([\n",
    "            [q_x,0,0,0,0],\n",
    "            [0,q_y,0,0,0],\n",
    "            [0,0,q_theta,0,0],\n",
    "            [0,0,0,q_vx,0],\n",
    "            [0,0,0,0,q_vy]\n",
    "            ])\n",
    "\n",
    "    B = np.array([[DELTA_T*THYMIO_MMS*np.cos(theta)/2, DELTA_T*THYMIO_MMS*np.cos(theta)/2],\n",
    "              [DELTA_T*THYMIO_MMS*np.sin(theta)/2, DELTA_T*THYMIO_MMS*np.sin(theta)/2],\n",
    "              [DELTA_T*THYMIO_MMS/D_BASELINE, -DELTA_T*THYMIO_MMS//D_BASELINE],\n",
    "              [1, 0],\n",
    "              [0, 1]])\n",
    "    \n",
    "    # Prediction Phase\n",
    "    mu_pred = np.dot(A,mu_pred_prev) + np.dot(B,u_state_prev)\n",
    "    sigma_pred = np.matmul(np.matmul(Jacobien(theta, v_r, v_l),sigma_pred_prev),np.transpose(Jacobien(theta, v_r, v_l))) + Q \n",
    "\n",
    "    y = np.array([[x],[y],[theta],[v_r],[v_l]])\n",
    "        \n",
    "    inno = y -np.dot(C, mu_pred)\n",
    "\n",
    "    S = np.matmul(np.matmul(C,sigma_pred),np.transpose(C)) + R \n",
    "\n",
    "    K_gain = np.matmul(np.matmul(sigma_pred,np.transpose(C)),np.linalg.inv(S))\n",
    "\n",
    "    # Posteriori\n",
    "\n",
    "    mu_post = mu_pred + np.dot(K_gain,inno)\n",
    "    sigma_post = np.matmul(np.subtract(np.eye(5),np.matmul(K_gain,C)),sigma_pred)\n",
    "\n",
    "    return mu_post, sigma_post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motion Control Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "motor_speed = 100\n",
    "count_control = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This control strategy will perform correction based on the angle difference between the desired orientation from current position toward the subsequent sub-goal and the current robot's position (angle_error = angle_target - angle_robot, hence |angle_error| < 2π since |angle_target|, |angler_robot| < π thanks to np.arctan2 and normalize_angle functions, respectively) where the higher the difference, the stronger the correction. There are 2 cases:\n",
    "* angle_error < π: if the difference is larger than 0, meaning the desired orientation is larger than the current one, the robot will rotate counter-clockwise by having the left motor's speed smaller than the right; and vice versa, if the difference is smaller than 0, meaning the desired orientation is smaller than the current one, the robot will rotate clockwise by having the left motor's speed larger than the right.\n",
    "* angle_error > π: the condition with π is to avoid the robot from rotating unnecessary larger angle (π + |Ѳ| > π) but more effective angle (2π - angle_error < π) to avoid dramatic movement and reduce the orientation variance of the odometry; the control strategy will be reversed from the first case where if the difference is larger than 0, the robot will rotate clockwise by having the left motor's speed larger than the right; and vice versa, if the difference is smaller than 0, the robot will rotate counter-clockwise by having the left motor's speed smaller than the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_control(angle_error, kp, motor_speed):\n",
    "   \"\"\"\n",
    "   Control the robot motors' speed to follow the predefined path by utilizing P controller\n",
    "   to compromise the angle error between desired orientation from current position toward subsequent sub-goal and current orientation.\n",
    "\n",
    "   Parameters:\n",
    "   angle_error (float): The input angle difference between desired orientation from current position toward sub-goal and current orientation.\n",
    "   kp (float): The input gain for P controller.\n",
    "   motor_speed (float): The input speed for both right and left wheel motors.\n",
    "   \n",
    "   Return:\n",
    "   float: The output right motor's speed.\n",
    "   float: The output left motor's speed.\n",
    "   \"\"\"\n",
    "   \n",
    "   # difference between the desired and current orientation is smaller than π\n",
    "   if abs(angle_error) < np.pi:\n",
    "      p_correction = kp * abs(angle_error) \n",
    "      if angle_error < 0.0:\n",
    "         motor_left = int(motor_speed + p_correction)\n",
    "         motor_right = int(motor_speed - p_correction)\n",
    "      else:\n",
    "         motor_left = int(motor_speed - p_correction)\n",
    "         motor_right = int(motor_speed + p_correction)\n",
    "      \n",
    "   # difference between the desired and current orientation is larger than π, replace by (2π - difference) and flip the control strategy\n",
    "   else:\n",
    "      p_correction = kp * abs(2 * np.pi - angle_error)\n",
    "      if angle_error > 0.0:\n",
    "         motor_left = int(motor_speed + p_correction)\n",
    "         motor_right = int(motor_speed - p_correction)\n",
    "      else:\n",
    "         motor_left = int(motor_speed - p_correction)\n",
    "         motor_right = int(motor_speed + p_correction)\n",
    "      \n",
    "   return motor_left, motor_right "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a variable to track the current sub-goal\n",
    "count_control = 0\n",
    "\n",
    "async def motion_control(pose, path):\n",
    "    \"\"\"\n",
    "    Control the motion of the robot by continuously correcting the angle difference\n",
    "    between current orientation and desired angle toward the subgoal using P controller.\n",
    "    \n",
    "    Parameters:\n",
    "    pose (tuple): The input current position.\n",
    "    path (list): The input simplified path as a list of tuples (critical sub-goals) and converted to real world in milimeter scale.\n",
    "    \n",
    "    Return:\n",
    "    Directly change the speed of left and right motors.\n",
    "    \"\"\"\n",
    "    \n",
    "    global count_control, left_speed, right_speed\n",
    "\n",
    "    # iterate the function until all sub-goals have been reached in order\n",
    "    if count_control < len(path):\n",
    "        sub_goal = path[count_control]\n",
    "        \n",
    "        dist_x = sub_goal[0] - pose[0] # distance in x-direction from current position to sub-goal \n",
    "        dist_y = sub_goal[1] - pose[1] # distance in y-direction from current position to sub-goal \n",
    "        angle_target = np.arctan2(dist_y, dist_x) # desired angle from current position toward sub-goal\n",
    "        \n",
    "        angle_robot = normalize_angle(pose[2]) # current angle\n",
    "        angle_error = normalize_angle(angle_target - angle_robot) # angle difference between desired angle from current position toward sub-goal and current angle\n",
    "        \n",
    "        dist_to_goal = np.sqrt((dist_x)**2 + dist_y**2) # distance from current pose to sub-goal\n",
    "\n",
    "        # first correction for the starting point \n",
    "        if sub_goal[0] == sub_goal[1] == 0 and abs(sub_goal[2] - angle_robot) > np.deg2rad(10):\n",
    "            left_speed, right_speed = p_control(sub_goal[2] - angle_robot, 45, motor_speed = 0)\n",
    "            await node.set_variables(motors(left_speed, right_speed))\n",
    "\n",
    "        elif dist_to_goal >= SUCCESS_THRESHOLD:\n",
    "            \n",
    "            # slow down when reaching the goal to have more accurate movement\n",
    "            if dist_to_goal <= 40:\n",
    "                left_speed, right_speed = p_control(angle_error, 55, 40)\n",
    "                await node.set_variables(motors(left_speed, right_speed))\n",
    "            \n",
    "            # move with larger speed to reduce traveling time\n",
    "            else:\n",
    "                left_speed, right_speed = p_control(angle_error, 80, 75)\n",
    "                await node.set_variables(motors(left_speed, right_speed))\n",
    "            \n",
    "        else:\n",
    "            # print('correction finished')\n",
    "            count_control += 1\n",
    "        # print(sub_goal)\n",
    "        # print('count: ', count_control)\n",
    "\n",
    "    else: \n",
    "        await node.set_variables(motors(0,0))\n",
    "\n",
    "    return left_speed, right_speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering Part\n",
    "x_odom_array = []\n",
    "y_odom_array = []\n",
    "\n",
    "\n",
    "sigma = np.zeros(5)\n",
    "x_kalman = []\n",
    "y_kalman = []\n",
    "\n",
    "#Initialisation des graphiques\n",
    "fig, ax, line = initialize_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corners found False\n",
      "corners found False\n",
      "corners found False\n",
      "corners found False\n",
      "corners found False\n",
      "corners found False\n",
      "corners found True\n",
      "corners found True\n",
      "corners found True\n",
      "corners found True\n",
      "corners found True\n",
      "corners found True\n",
      "corners found True\n",
      "corners found True\n",
      "corners found True\n",
      "corners found True\n",
      "corners found True\n",
      "corners found True\n",
      "corners found True\n",
      "corners found True\n",
      "corners found True\n",
      "corners found True\n",
      "corners found True\n",
      "corners found True\n",
      "corners found True\n",
      "corners found True\n",
      "corners found True\n",
      "corners found True\n",
      "corners found True\n",
      "corners found True\n",
      "pose vision:  (0.0, 0.0, -2.6147742835038703)\n",
      "pose vision:  (4.447263377449304e-17, 1.5602224653035644e-17, 2.6689966050114773)\n",
      "pose vision:  (-1.2475372778488314e-17, -1.7871583215441752e-16, 3.0816865517723997)\n",
      "pose vision:  (1.6799239519782093e-17, -6.657284048971119e-17, -2.6335154651505563)\n",
      "pose vision:  (-0.10265485850972111, -1.6285537643749188, -2.819965219164438)\n",
      "pose vision:  (-0.1086397555485663, -2.7691412414139798, -2.6687362919346205)\n",
      "pose vision:  (-0.11420724568470035, -3.8161496783702504, -2.5379629318602173)\n",
      "pose vision:  (-0.025963911902529863, -3.965031994050272, -2.496737313324693)\n",
      "pose vision:  (-0.005889627705737206, -3.9935361232641746, -2.397851730165449)\n",
      "pose vision:  (0.7857273781758517, -4.809272195975476, -2.2980383242113374)\n",
      "pose vision:  (0.9559071256003981, -4.962391006349665, -2.276246251349614)\n",
      "pose vision:  (0.9906749617585089, -4.992958730965394, -2.214879898190055)\n",
      "pose vision:  (-0.894190511093889, -12.208662526041259, 2.671281840987371)\n",
      "pose vision:  (-0.04488498580872613, -14.429384700477243, -2.6388796420827445)\n",
      "pose vision:  (-0.0010092548814905483, -14.884692510784694, -3.102546916358666)\n",
      "pose vision:  (3.9381674411566006, -6.234584717511206, -1.6207134183140859)\n",
      "pose vision:  (5.58559981685505, -6.0585417261724945, -1.440647756920903)\n",
      "pose vision:  (5.916316090159938, -6.012206080521913, -1.4266637120192813)\n",
      "pose vision:  (5.945369357074266, -5.031359919745462, -1.360298792289764)\n",
      "pose vision:  (6.027283189942919, -5.974662814212971, -1.3503218463144449)\n",
      "pose vision:  (5.962514995976711, -5.028800681980083, -1.3221641892123932)\n",
      "pose vision:  (5.991556297924396, -5.002733934312355, -1.2979269459450755)\n",
      "pose vision:  (5.998286473842391, -5.000529006556593, -1.274287502294151)\n",
      "pose vision:  (5.999653630930115, -5.000106611241529, -1.2724519234001)\n",
      "pose vision:  (6.0589589841776474, -5.958783930737427, -1.222049068320234)\n",
      "pose vision:  (5.950771766215918, -5.038626431789163, -1.1969484386009723)\n",
      "pose vision:  (5.9888552901871615, -5.0047750106673305, -1.1949994456862907)\n",
      "pose vision:  (6.824326391842734, -5.067367220936564, -1.1651592602788383)\n",
      "pose vision:  (6.9650708063455555, -5.01493637982358, -1.162842273428285)\n",
      "pose vision:  (6.9930860406653546, -5.003315567332663, -1.1214982975397696)\n",
      "pose vision:  (6.998627453630454, -5.000716714636052, -1.0879946517020467)\n",
      "pose vision:  (6.999723208322787, -5.000145936138359, -1.0853932273963083)\n",
      "pose vision:  (6.999944658505559, -5.000030560335092, -1.065382379841484)\n",
      "pose vision:  (6.999988898372359, -5.000006319415729, -1.0527164570427034)\n",
      "pose vision:  (6.999997758439548, -5.00000128042132, -1.0517330018554985)\n",
      "pose vision:  (6.999999552093891, -5.0000002672178585, -1.0320350410446189)\n",
      "pose vision:  (6.999999909607677, -5.000000054205923, -1.0305055796514813)\n",
      "pose vision:  (6.999999982308469, -5.000000011803567, -0.9802663103967713)\n",
      "pose vision:  (6.999999996438739, -5.0000000024059155, -0.9763654512692876)\n",
      "pose vision:  (7.8585356196917315, -5.085958000201287, -0.9577043062849819)\n",
      "pose vision:  (7.972875441928527, -5.019383921893131, -0.9432194071267768)\n",
      "pose vision:  (7.9944992740668015, -5.003889898811652, -0.9551306625374982)\n",
      "pose vision:  (7.912100448226109, -4.083456692767786, -0.9430195748830847)\n",
      "pose vision:  (8.573860183655821, -3.5684886693995006, -0.7379391413642051)\n",
      "pose vision:  (8.804269113697213, -4.363940195576937, -0.7043471950922)\n",
      "pose vision:  (8.765892444609555, -4.411290857321159, -0.7017389389435662)\n",
      "pose vision:  (9.675057491126244, -5.4918031159700496, -0.7602693396445224)\n",
      "pose vision:  (9.92592895907222, -7.211777486058116, -0.6686958092269584)\n",
      "pose vision:  (9.855482142063714, -8.38787280998339, -0.7658825335261463)\n",
      "pose vision:  (9.92436317454484, -10.309336465649118, -0.756147629127013)\n",
      "pose vision:  (9.697038806438398, -10.606350692680143, -0.897626275340409)\n",
      "pose vision:  (9.823310701272089, -12.468914086043535, -0.8635081763464192)\n",
      "pose vision:  (10.453528265367844, -12.758514726103108, -0.9737232939835883)\n",
      "pose vision:  (11.274891243466834, -13.868941265922041, -1.1470444396079391)\n",
      "pose vision:  (11.398500165188274, -15.843445191912426, -1.2277585345096151)\n",
      "pose vision:  (11.317143190919008, -15.910132883491416, -1.2569288061786206)\n",
      "pose vision:  (11.303008987510294, -17.8942346668398, -1.3268030024054753)\n",
      "pose vision:  (11.269249985377565, -18.919711704409497, -1.332228422027463)\n",
      "pose vision:  (11.22701718661739, -19.935351293637904, -1.3679890019289986)\n",
      "pose vision:  (12.024449091507815, -20.962088443771947, -1.3662345913652423)\n",
      "pose vision:  (12.181706452187065, -21.941362356464083, -1.3660983692682844)\n",
      "pose vision:  (12.213409301105454, -22.936662948240663, -1.3660877922716235)\n",
      "pose vision:  (12.1933610883736, -23.945054666506937, -1.3931569879738508)\n",
      "pose vision:  (12.253080376241526, -25.908969397320487, -1.3584301891505652)\n",
      "pose vision:  (12.209954265415226, -25.954247595801146, -1.3557338082467754)\n",
      "pose vision:  (10.05107217037817, -38.81981488995146, 0.9437353794805086)\n",
      "pose vision:  (10.772110429283105, -40.65180392155783, 1.069981300130924)\n",
      "pose vision:  (11.042877684070431, -40.92945522835319, 1.1083712172323992)\n",
      "pose vision:  (11.885138662352743, -41.790824629249975, 1.1086761796546707)\n",
      "pose vision:  (12.050453242527889, -41.93030446229859, 1.0843681796040885)\n",
      "pose vision:  (10.144542095387456, -42.19744598823879, -2.2380598630544473)\n",
      "pose vision:  (9.489328203118061, -35.39145635629704, -2.129028899330824)\n",
      "pose vision:  (8.089025808096908, -35.106546509994814, -2.2476773559984555)\n",
      "pose vision:  (7.767703921612328, -35.91898501671887, -2.259632472604428)\n",
      "pose vision:  (7.69748143847773, -36.8907255673735, -2.1937372771899515)\n",
      "pose vision:  (6.868586934933105, -37.96600163452244, -2.0824221960353126)\n",
      "pose vision:  (6.791838866976411, -37.96773120806099, -2.0170447768600344)\n",
      "pose vision:  (6.689460706636639, -40.89468861386936, -1.8857632061062293)\n",
      "pose vision:  (6.777029128977054, -41.9580299570299, -1.8148750108416445)\n",
      "pose vision:  (6.896131538612016, -41.99858310256397, -1.6781500092988524)\n",
      "pose vision:  (6.931424543038658, -42.98152465198425, -1.6367881143506802)\n",
      "pose vision:  (6.9859540422192055, -43.98004979907651, -1.5759202945336304)\n",
      "pose vision:  (6.974912570927341, -44.981403423408565, -1.5976362905940698)\n",
      "pose vision:  (7.018268578200558, -45.97784092177764, -1.5458758124171887)\n",
      "pose vision:  (7.075946569430522, -46.96997029026046, -1.4879586653478527)\n",
      "pose vision:  (7.12829913544996, -48.942678510182695, -1.456622856423268)\n",
      "pose vision:  (7.09884235245619, -49.96784815999079, -1.48102857171002)\n",
      "pose vision:  (7.929672837349331, -51.95786613634449, -1.4583175195769658)\n",
      "pose vision:  (8.060297456094657, -52.97018692862854, -1.4794069871631292)\n",
      "pose vision:  (8.0600719323792, -53.97371620377995, -1.5097190725721492)\n",
      "pose vision:  (8.858133189709694, -54.98157294277352, -1.5120726707012564)\n",
      "pose vision:  (9.018331108759856, -55.9756413149949, -1.5132815614368709)\n",
      "pose vision:  (9.043103481121296, -55.99419883614567, -1.5133754264064168)\n",
      "pose vision:  (9.177743514787108, -59.89615501722819, -1.4396409353354227)\n",
      "pose vision:  (9.245254580197903, -60.9238131257437, -1.3405882474106345)\n",
      "pose vision:  (15.750379977312583, -69.8725705101603, 2.932027693134744)\n",
      "pose vision:  (17.06796369508225, -70.66219103068795, 0.08920248322485103)\n",
      "pose vision:  (15.259928618260885, -63.926216428501604, -0.5478040711897556)\n",
      "pose vision:  (16.57028820359064, -63.32116022500559, -0.42370384737132394)\n",
      "pose vision:  (17.654641655526373, -63.236980465209726, -0.434713802945224)\n",
      "pose vision:  (17.85351750905659, -63.976062855199835, -0.41154102074036025)\n",
      "pose vision:  (18.796709535364492, -64.9917466338929, -0.39192185184216743)\n",
      "pose vision:  (19.69564100446178, -65.10388118836931, -0.3686352321276467)\n",
      "pose vision:  (20.764955322677036, -65.99452916021706, -0.3668271300504289)\n",
      "pose vision:  (20.447012913535968, -62.67671781452879, -0.5978351154100814)\n",
      "pose vision:  (20.711162424249846, -62.376847767413075, -0.6092951132272328)\n",
      "pose vision:  (21.643870545614785, -62.42577655276284, -0.6208313219233972)\n",
      "pose vision:  (21.762007750986783, -62.2985002580137, -0.5793061878950878)\n",
      "pose vision:  (20.13203437133261, -54.08357739276224, -0.795922887146018)\n",
      "pose vision:  (26.193516354100222, -64.45960654192716, -0.5263984359722884)\n",
      "pose vision:  (26.800388273211258, -65.15457678529476, -0.5058175146926849)\n",
      "pose vision:  (27.786146378674797, -66.11810902570573, -0.47520960259900313)\n",
      "pose vision:  (28.852210043725176, -67.82135107955534, -0.34855688493370574)\n",
      "pose vision:  (29.708345046774088, -68.05083878403151, -0.34293956839891626)\n",
      "pose vision:  (33.67686528161655, -71.43639928590967, -0.11970899567552484)\n",
      "pose vision:  (33.67464042461146, -72.52679006079981, -0.06718728364464654)\n",
      "pose vision:  (35.512727537377806, -72.8568831883689, -0.0738062263890491)\n",
      "pose vision:  (36.62254311236977, -73.7049201865428, -0.09881199298884713)\n",
      "pose vision:  (37.66128661779085, -74.69311763785461, -0.1248859449689177)\n",
      "pose vision:  (38.582225835227604, -74.84468228206114, -0.11454568276509791)\n",
      "pose vision:  (39.68826581884902, -75.767224385801, -0.1759413241305232)\n",
      "pose vision:  (40.589162643764624, -75.86491294452797, -0.1308746932527689)\n",
      "pose vision:  (42.62459913876082, -76.84762008607922, -0.17720919890770093)\n",
      "pose vision:  (42.646639680616026, -76.81809458385656, -0.14323774381049947)\n",
      "pose vision:  (43.665636780110525, -77.72167622471322, -0.14060001195931227)\n",
      "pose vision:  (45.52620242421405, -77.92102938661822, -0.11576580992461327)\n",
      "pose vision:  (46.62995509890842, -78.72514377140728, -0.11383754498811172)\n",
      "pose vision:  (47.56729830402714, -78.8424969334448, -0.10048930041453819)\n",
      "pose vision:  (48.614892791410625, -79.68792432376758, -0.08763152167552235)\n",
      "pose vision:  (49.56852275112705, -79.83709376984123, -0.09845451967916041)\n",
      "pose vision:  (50.60192124868545, -80.6757860552756, -0.07332994645951185)\n",
      "pose vision:  (51.51276413765602, -80.79531554145088, -0.03440394264008617)\n",
      "pose vision:  (59.21238630018586, -90.9500908055011, 1.5098967904658052)\n",
      "pose vision:  (61.28481146574552, -91.11982616356639, 1.6609515554731997)\n",
      "pose vision:  (62.54555722814666, -91.05818992288886, 1.6412763419475267)\n",
      "pose vision:  (62.970486828142526, -91.97634190415731, -1.2508224302260764)\n",
      "pose vision:  (57.349129495561385, -89.05518776626705, -0.688400059676546)\n",
      "pose vision:  (56.95051646622612, -90.26651087874717, -0.7118842699361321)\n",
      "pose vision:  (57.66958575316413, -90.44603973143776, -0.6738174304097249)\n",
      "pose vision:  (58.7281013595435, -91.32040270044449, -0.6095830132756985)\n",
      "pose vision:  (59.66843168195815, -91.3340165012528, -0.5461533567964434)\n",
      "pose vision:  (60.664415089858416, -91.29532460327577, -0.501646958095681)\n",
      "pose vision:  (61.755346336256665, -92.07770852189014, -0.42011995356941023)\n",
      "pose vision:  (62.7744804320454, -92.99230216067807, -0.375488279208402)\n",
      "pose vision:  (63.68389827679602, -93.0699118670336, -0.33432690812061017)\n",
      "pose vision:  (64.66387029994341, -93.07176286498547, -0.31562188409338976)\n",
      "pose vision:  (65.65387614028431, -93.05151948729497, -0.2926664731867863)\n",
      "pose vision:  (41.95322454825536, -67.99453682749426, -0.4711965261860196)\n",
      "pose vision:  (42.54330464357737, -66.45291437124013, -0.49007397886854953)\n",
      "pose vision:  (70.112364961142, -92.0321432410388, -0.022243567585170876)\n",
      "pose vision:  (70.70878723735875, -95.87907394360028, 0.039149658409499466)\n",
      "pose vision:  (71.51598348999823, -96.62188946784266, 0.01884828557381013)\n",
      "pose vision:  (73.42708436645563, -96.82524292525952, 0.017271974591593242)\n",
      "pose vision:  (73.49121079471682, -96.75317334528557, 0.01714958107994402)\n",
      "pose vision:  (75.47568498818687, -97.66852159892427, 0.017140077770087636)\n",
      "pose vision:  (76.47803027931725, -97.786281491174, 0.0014245416346856388)\n",
      "pose vision:  (77.45623004498309, -97.80029354068662, 0.015919098648431884)\n",
      "pose vision:  (78.44568354139102, -97.80047681155065, 0.025595886471162288)\n",
      "pose vision:  (78.5692128840287, -98.57237361174478, 0.002081097436446022)\n",
      "pose vision:  (79.49707373046422, -98.76803702760145, -0.008866002817139673)\n",
      "pose vision:  (80.44970838020546, -98.79236870569522, 0.023671433974684852)\n",
      "pose vision:  (82.40695247094683, -98.8552706856061, 0.02619782070543497)\n",
      "pose vision:  (83.45959236626044, -99.61566883221754, 0.058439844565794985)\n",
      "pose vision:  (84.40304231922005, -99.74952628120477, 0.07803242147304612)\n",
      "pose vision:  (85.3941504799831, -99.77744041143114, 0.07955369761524533)\n",
      "pose vision:  (86.4028446291382, -100.58594858634642, 0.1108630826843422)\n",
      "pose vision:  (88.26158683527977, -100.76957679024157, 0.18191682301292023)\n",
      "pose vision:  (88.32529164218303, -100.74113632500043, 0.1675089158494627)\n",
      "pose vision:  (89.30168493490726, -100.7690397066684, 0.16639020617025402)\n",
      "pose vision:  (90.27171836519054, -101.58206012701433, 0.2151623154042861)\n",
      "pose vision:  (91.15810425714453, -101.76721123328751, 0.293762575069346)\n",
      "pose vision:  (92.17574631854134, -101.7981635264619, 0.27854291959622124)\n",
      "pose vision:  (93.13638702600927, -101.01423463386634, 0.3004084710341717)\n",
      "pose vision:  (94.17616152215022, -100.84638094433153, 0.27431886656932747)\n",
      "pose vision:  (95.2635707998243, -101.60142832168454, 0.22354249412716776)\n",
      "pose vision:  (96.24215030045544, -101.74864286017217, 0.22326339512276494)\n",
      "pose vision:  (96.26561433492587, -101.7503371817179, 0.21957826466373387)\n",
      "pose vision:  (97.29837943511905, -101.77330583113589, 0.1704331586856842)\n",
      "pose vision:  (98.39761642136705, -102.58945987759988, 0.11791946718122759)\n",
      "pose vision:  (101.48218882098696, -103.74527665826055, -0.024447286091015208)\n",
      "pose vision:  (102.6278296122789, -104.68803668991444, -0.09497949159635422)\n",
      "pose vision:  (103.5707043753855, -104.837338611865, -0.10045600252188702)\n",
      "pose vision:  (104.54403452086771, -105.63697177116741, -0.016827550937904334)\n",
      "pose vision:  (106.39695183422745, -105.80932877821803, 0.047529898922031855)\n",
      "pose vision:  (107.29102371496313, -104.9728048667488, 0.1326889122934869)\n",
      "pose vision:  (108.26652326459566, -105.62266407822152, 0.21254556036595496)\n",
      "pose vision:  (110.12460825277006, -104.98981753238165, 0.29174554880620374)\n",
      "pose vision:  (111.09622595502184, -104.87645779539483, 0.3465658172834356)\n",
      "pose vision:  (113.02542286067566, -104.88178053518085, 0.39693048535698505)\n",
      "pose vision:  (113.03777371807178, -104.91600048374139, 0.42469877799586886)\n",
      "pose vision:  (114.01892016004189, -104.91320938670466, 0.4256978284528721)\n",
      "pose vision:  (115.04209150325137, -104.8942960165219, 0.40307473280570605)\n",
      "pose vision:  (117.09843709921455, -104.85484997767351, 0.3318601192831965)\n",
      "pose vision:  (119.23164931216694, -105.63264824669182, 0.23167377522281463)\n",
      "pose vision:  (120.36902289348477, -106.56025781533664, 0.14698686233213998)\n",
      "pose vision:  (122.2830736805067, -106.76818199336945, 0.16479676031815238)\n",
      "pose vision:  (123.27177773512685, -106.77691045958922, 0.19060558320424192)\n",
      "pose vision:  (125.13535681478483, -106.82027085163834, 0.2918558544269745)\n",
      "pose vision:  (126.09243630620047, -106.04413858113308, 0.34657438202252067)\n",
      "pose vision:  (128.0027720319543, -105.92967792828821, 0.41963181782647485)\n",
      "pose vision:  (128.01641427544004, -105.94882706431633, 0.44773334861982406)\n",
      "pose vision:  (129.91171547875945, -105.9805609209662, 0.5153294298059965)\n",
      "pose vision:  (130.93060143545122, -105.26098503498496, 0.5666214305859789)\n",
      "pose vision:  (131.9304852687229, -104.34144295892756, 0.5913719123898238)\n",
      "pose vision:  (133.83357809554124, -104.17676980835101, 0.6391992967388838)\n",
      "pose vision:  (134.9040146425066, -103.37876085314343, 0.6429128796482875)\n",
      "pose vision:  (135.843049988701, -103.3140381013297, 0.7011775261461484)\n",
      "pose vision:  (137.84834983371675, -102.44239491830969, 0.7240467016888745)\n",
      "pose vision:  (138.83231478566842, -102.38797844724385, 0.7512428753917444)\n",
      "pose vision:  (138.9425917949451, -101.61800054729076, 0.7533545368852348)\n",
      "pose vision:  (139.84619218015882, -101.45114388927179, 0.7711252904666592)\n",
      "pose vision:  (140.83845990828436, -101.35341297977662, 0.7294778333651548)\n",
      "pose vision:  (141.8928302029194, -100.53529740553527, 0.7440967566039838)\n",
      "pose vision:  (142.9071017013266, -99.60378038470971, 0.7704064651514253)\n",
      "pose vision:  (144.79419966600253, -99.36727452356351, 0.7646362354361811)\n",
      "pose vision:  (145.89058893796158, -98.59118561234943, 0.7890953458340011)\n",
      "pose vision:  (146.84010824259744, -98.46898002919473, 0.7909944865791783)\n",
      "pose vision:  (147.9026689909886, -97.64457240816373, 0.8162812187886526)\n",
      "pose vision:  (148.92448225342815, -96.70994472944348, 0.8435632926576542)\n",
      "pose vision:  (150.86698623595174, -95.65420616543719, 0.8550932807395784)\n",
      "pose vision:  (151.92679836798928, -94.72874874734698, 0.8624673413345647)\n",
      "pose vision:  (152.9396036955567, -93.75652155659269, 0.8725914106565593)\n",
      "pose vision:  (153.87553215341882, -93.68464775791098, 0.9284184087786578)\n",
      "pose vision:  (155.03144449586233, -91.93591172045781, 0.9182549877336843)\n",
      "pose vision:  (155.89892897165527, -91.71852753638137, 0.9319639817119008)\n",
      "pose vision:  (156.96159037280256, -90.83081531504274, 0.9470059409489293)\n",
      "pose vision:  (157.97550143086545, -89.84831394464807, 0.944727342210177)\n",
      "pose vision:  (158.9904655964293, -88.87878586037591, 0.9720617726557608)\n",
      "pose vision:  (159.99485963538723, -87.88455927269831, 0.9741841691681881)\n",
      "pose vision:  (161.0009295408372, -86.89569294409344, 0.9852811851955732)\n",
      "pose vision:  (162.9102566610796, -85.81741720143808, 0.997428631487896)\n",
      "pose vision:  (163.1002548419774, -84.9929213321433, 1.0125680405848403)\n",
      "pose vision:  (164.04588830121844, -83.957778420037, 1.0365435487205632)\n",
      "pose vision:  (165.93226920497477, -82.86614682587572, 1.0503179547105788)\n",
      "pose vision:  (166.1416012168018, -82.036225617291, 1.0632247711595628)\n",
      "pose vision:  (167.06739294226503, -80.9828369188314, 1.0646558760975022)\n",
      "pose vision:  (168.06495695528835, -79.98357160703792, 1.0877628003048274)\n",
      "pose vision:  (169.1650439562926, -78.08996443503321, 1.112612148548397)\n",
      "pose vision:  (170.0921903021502, -77.0089088491784, 1.1022674955742708)\n",
      "pose vision:  (171.0900192028398, -76.00864856340246, 1.1363839066572927)\n",
      "pose vision:  (172.09465459955135, -75.0127796417067, 1.1517753498317447)\n",
      "pose vision:  (172.22434686523354, -74.0981133739896, 1.1402279705540384)\n",
      "pose vision:  (173.1159698688922, -73.02854649535978, 1.139331368096725)\n",
      "pose vision:  (174.18982928027083, -71.10214504563481, 1.1614619511090298)\n",
      "pose vision:  (175.12268171285695, -70.03381941757162, 1.1722288587996328)\n",
      "pose vision:  (176.11327612561402, -69.0269068045808, 1.198685953119055)\n",
      "pose vision:  (176.25390337026192, -68.10849710985882, 1.187794441835221)\n",
      "pose vision:  (177.13909798037466, -67.04337900767106, 1.1998945511622594)\n",
      "pose vision:  (177.25903667077253, -66.11194829879217, 1.187888284078502)\n",
      "pose vision:  (178.13071132894746, -65.03887634798018, 1.1652321788933317)\n",
      "pose vision:  (179.1062227824323, -64.02157452598456, 1.1634730334549097)\n",
      "pose vision:  (180.10132034803422, -63.01801851036697, 1.163336443663848)\n",
      "pose vision:  (180.23187824542327, -62.101625990135695, 1.150751219193543)\n",
      "pose vision:  (180.16186342880985, -62.02943065413018, 1.1401484513813536)\n",
      "pose vision:  (181.02256414042444, -61.940511661825056, 1.1615253939788968)\n",
      "pose vision:  (180.26218030442695, -62.08981814229999, 1.150610599269421)\n",
      "pose vision:  (180.17188728003015, -62.035747178630395, 1.149763114180189)\n",
      "pose vision:  (180.1549009060878, -62.02315890360839, 1.149697310745898)\n",
      "pose vision:  (180.14468348220078, -62.015657958736874, 1.1400666200946858)\n",
      "pose vision:  (180.14923565056583, -62.01892653930591, 1.1489444212620334)\n",
      "pose vision:  (181.01615026246114, -61.93361471204331, 1.1527506177249052)\n",
      "pose vision:  (180.2604421249808, -62.08856260824419, 1.149929277060938)\n",
      "pose vision:  (181.04154149104875, -61.954086521831556, 1.1622848314346124)\n",
      "pose vision:  (180.2568118052554, -62.08816329614912, 1.1410439848897216)\n",
      "pose vision:  (180.16376847656915, -62.030330777369116, 1.1393947279674794)\n",
      "pose vision:  (181.02290775560718, -61.94075248741998, 1.1614668707201767)\n",
      "pose vision:  (180.262248304391, -62.08986912905448, 1.1506060551996082)\n",
      "pose vision:  (180.1648613675505, -62.03107679788553, 1.1401371800451887)\n",
      "pose vision:  (180.1464943637347, -62.01712366173127, 1.1393243186189714)\n",
      "pose vision:  (180.14281939459391, -62.0142599909951, 1.1392612035575649)\n",
      "pose vision:  (181.01471347494365, -61.932528257336585, 1.151998759092768)\n",
      "pose vision:  (180.25107665433845, -62.083773805594774, 1.140245317285931)\n",
      "pose vision:  (180.16261926204433, -62.02943435935959, 1.139332714993027)\n",
      "pose vision:  (180.15287723301827, -62.02169045382277, 1.1488874368067963)\n",
      "pose vision:  (181.01688913366624, -61.934169625156926, 1.1527461931363145)\n",
      "pose vision:  (180.2515095425916, -62.08411122939278, 1.1403033522031203)\n",
      "pose vision:  (180.16270646699627, -62.02950252489673, 1.1393372211452988)\n",
      "pose vision:  (180.1460701689257, -62.01679405780741, 1.1392622053811419)\n",
      "pose vision:  (180.14952489278292, -62.019147127414044, 1.1488819620501634)\n",
      "pose vision:  (180.1436150974732, -62.01482711542842, 1.140003311907905)\n",
      "pose vision:  (180.14901811679152, -62.018761171185645, 1.1489395056639164)\n",
      "pose vision:  (180.14351333923182, -62.01474790949313, 1.140007779912657)\n",
      "pose vision:  (181.01484982320704, -61.932629881293295, 1.1520567274142763)\n",
      "pose vision:  (180.26018074664773, -62.08836407324242, 1.1498753995756088)\n",
      "pose vision:  (180.17148998452308, -62.03544701826919, 1.1497060292056487)\n",
      "pose vision:  (180.1479771087772, -62.01821636091565, 1.1400672970441708)\n",
      "pose vision:  (180.1499066411624, -62.019436425642844, 1.148944473824141)\n",
      "pose vision:  (181.0162864321347, -61.93371698983566, 1.1527506218061188)\n",
      "pose vision:  (180.2513893164452, -62.084017739209735, 1.1403036960695623)\n",
      "pose vision:  (180.16969741852324, -62.03419349284734, 1.148962829153259)\n",
      "pose vision:  (180.15446917356815, -62.02283350093071, 1.149635172185759)\n",
      "pose vision:  (180.151406182196, -62.020516791378704, 1.1496873766211202)\n",
      "pose vision:  (180.13668582135492, -62.009484492598496, 1.130009244486664)\n",
      "pose vision:  (180.1473870975075, -62.017572471035706, 1.1481635109388026)\n",
      "pose vision:  (180.14319262958855, -62.01449883070204, 1.1399475273860293)\n",
      "pose vision:  (181.0147844359831, -61.93258064353062, 1.1520520490745314)\n",
      "pose vision:  (180.26016756355224, -62.08835409177622, 1.1498750363234107)\n",
      "pose vision:  (180.09090419131343, -62.95582986636208, 1.158924221786525)\n",
      "pose vision:  (181.0904070455274, -62.00848768416384, 1.1535255046891706)\n",
      "pose vision:  (180.26786332555076, -62.09720817333309, 1.1403638622666161)\n",
      "pose vision:  (181.043164263264, -61.95533328695521, 1.1615421196726157)\n",
      "pose vision:  (180.26627552724844, -62.09290267332447, 1.1506118979448807)\n",
      "pose vision:  (181.04272759664332, -61.95495177314889, 1.1623378338992616)\n",
      "pose vision:  (180.26618093018638, -62.09283088496221, 1.1506736816027807)\n",
      "pose vision:  (180.17269395161918, -62.03635800330638, 1.149768012241796)\n",
      "pose vision:  (180.15506389307012, -62.0232823740184, 1.1496976910585168)\n",
      "pose vision:  (180.1515260128428, -62.02060759006312, 1.1496922309325281)\n",
      "pose vision:  (180.15081154069995, -62.02006642816058, 1.149691806978117)\n",
      "pose vision:  (180.1506671652829, -62.01995705490823, 1.1496917740599493)\n",
      "pose vision:  (180.15063798925434, -62.01993495189805, 1.149691771504)\n",
      "pose vision:  (180.15063209320033, -62.019930485191324, 1.1496917713055428)\n",
      "pose vision:  (180.14383006094738, -62.01499372354046, 1.1400661899818694)\n",
      "pose vision:  (180.1422752915739, -62.01383424833028, 1.1393188065574318)\n",
      "pose vision:  (180.14196738318398, -62.01359448735231, 1.139260775570591)\n",
      "pose vision:  (180.1486879452119, -62.01851088794134, 1.1488818510317573)\n",
      "pose vision:  (180.1502442559778, -62.019637325607945, 1.1496288845960834)\n",
      "pose vision:  (180.14375274518704, -62.01493350259916, 1.140061307109585)\n",
      "pose vision:  (180.14225968759297, -62.013822060346676, 1.1393184274241932)\n",
      "pose vision:  (180.14196423003156, -62.01359202401051, 1.139260746132587)\n",
      "pose vision:  (180.14868730159714, -62.01851039858862, 1.1488818487460275)\n",
      "pose vision:  (180.15024412575707, -62.0196372269218, 1.149628884418605)\n",
      "pose vision:  (180.14375271913516, -62.0149334823185, 1.140061307095804)\n",
      "pose vision:  (180.14904590525092, -62.018782322022986, 1.1489440087313874)\n",
      "pose vision:  (181.01611175324186, -61.93358578697617, 1.1527505856937417)\n",
      "pose vision:  (180.2513544717766, -62.083990642373365, 1.140303693265598)\n",
      "pose vision:  (180.1626751792988, -62.02947809223066, 1.139337247627278)\n",
      "pose vision:  (180.14606384709714, -62.01678911908631, 1.1392622074373504)\n",
      "pose vision:  (180.14952360240198, -62.019146146312174, 1.1488819622098188)\n",
      "pose vision:  (180.14361483903647, -62.014826914180034, 1.1400033119203012)\n",
      "pose vision:  (180.14223212304012, -62.01380050580981, 1.1393139243566166)\n",
      "pose vision:  (180.14874171412842, -62.0185517983689, 1.1488859777977538)\n",
      "pose vision:  (180.1502551328871, -62.0196455702257, 1.1496292050210428)\n",
      "pose vision:  (180.15055498807, -62.01987203483901, 1.1496869132974155)\n",
      "pose vision:  (180.14381463813487, -62.0149817159798, 1.1400658127792234)\n",
      "pose vision:  (180.14905852733204, -62.018791917144874, 1.1489443585775927)\n",
      "pose vision:  (180.14352143011098, -62.01475421138851, 1.140008156719741)\n",
      "pose vision:  (180.1422132662904, -62.013785780192805, 1.139314300533667)\n",
      "pose vision:  (180.14195485219327, -62.013584696396535, 1.1392604256979588)\n",
      "pose vision:  (180.92690989214637, -62.847039224989935, 1.1519986986954418)\n",
      "pose vision:  (180.32858427597355, -62.159375958521764, 1.1498708939046125)\n",
      "pose vision:  (180.18699925233565, -62.04757213143008, 1.149705679360407)\n",
      "pose vision:  (180.15111277953574, -62.02066474397664, 1.140067269880248)\n",
      "pose vision:  (180.15054656764374, -62.01992295217221, 1.1489444717149837)\n",
      "pose vision:  (181.01641631924682, -61.93381455355341, 1.152750621642351)\n",
      "pose vision:  (180.26049574445432, -62.08860318708049, 1.1499292773651106)\n",
      "pose vision:  (180.17155348099243, -62.03549518830183, 1.1497102125755099)\n",
      "pose vision:  (181.0245895644345, -61.94197426712936, 1.1622678220551492)\n",
      "pose vision:  (180.2534845329864, -62.08558973004366, 1.1410426641873315)\n",
      "pose vision:  (181.036045294114, -61.94831190570264, 1.1521370815588305)\n",
      "pose vision:  (180.25531015172646, -62.087061443701984, 1.1402560574079974)\n",
      "pose vision:  (180.1704962170212, -62.034800412628144, 1.1489591302238296)\n",
      "pose vision:  (180.14778913275052, -62.01807180147012, 1.1400093036721355)\n",
      "pose vision:  (180.14307423919453, -62.01445809430054, 1.1393143895894013)\n",
      "pose vision:  (180.14891357758376, -62.018682464014304, 1.1488860139210022)\n",
      "pose vision:  (180.14349267074672, -62.014731783646326, 1.1400036265175881)\n",
      "pose vision:  (180.14220746240866, -62.013781245742194, 1.1393139487836912)\n",
      "pose vision:  (180.14873668098568, -62.018547971670465, 1.1488859796944073)\n",
      "pose vision:  (180.15025411455517, -62.01964479849729, 1.14962920516831)\n",
      "pose vision:  (180.14375471767028, -62.01493503814834, 1.1400613320005863)\n",
      "pose vision:  (180.1490463127061, -62.018782631713805, 1.148944010665132)\n",
      "pose vision:  (180.1435189838741, -62.01475230650749, 1.1400081297058922)\n",
      "pose vision:  (181.01485097790734, -61.9326307512495, 1.1520567545741596)\n",
      "pose vision:  (180.2511037595122, -62.083794963980814, 1.140249820376189)\n",
      "pose vision:  (180.16262473358307, -62.02943863482236, 1.1393330646378867)\n",
      "pose vision:  (180.15287834996903, -62.021691303073716, 1.1488874639551572)\n",
      "pose vision:  (180.1510915872775, -62.02027935656863, 1.1496293204145314)\n",
      "pose vision:  (180.14392225216866, -62.01506545695855, 1.1400613409489413)\n",
      "pose vision:  (180.14908046838917, -62.01880859211384, 1.148944011359931)\n",
      "pose vision:  (180.15032340129943, -62.0196973414485, 1.1496337110681036)\n",
      "pose vision:  (180.06976677778096, -62.94051527136099, 1.1589054839576445)\n",
      "pose vision:  (180.21476028610223, -62.08954146243922, 1.1407815936424015)\n",
      "pose vision:  (180.15830197974546, -62.02672999537442, 1.1393743544583508)\n",
      "pose vision:  (180.14523914420167, -62.01615793066349, 1.13926508861715)\n",
      "pose vision:  (180.14935641470802, -62.019019284447744, 1.1488821859205718)\n",
      "pose vision:  (180.15037951826932, -62.019739835313636, 1.1496289105987056)\n",
      "pose vision:  (180.15058012751555, -62.01989108052965, 1.1496868904368345)\n",
      "pose vision:  (181.01641709642175, -61.93381396285323, 1.1528082671421966)\n",
      "pose vision:  (180.26049554706367, -62.08860311233223, 1.1499337532808624)\n",
      "pose vision:  (181.0374250217178, -61.94930445439765, 1.1528274349401233)\n",
      "pose vision:  (180.18612938468118, -63.01195992944258, 1.1591534623606012)\n",
      "pose vision:  (180.23656258743878, -62.106173003401324, 1.1408008480584995)\n",
      "pose vision:  (180.16963443889108, -62.03479969622943, 1.1490014307860807)\n",
      "pose vision:  (181.02032241513112, -61.93675568393028, 1.152755044259976)\n",
      "pose vision:  (180.2521950425032, -62.0846444547898, 1.1403040394533646)\n",
      "pose vision:  (180.1698616391107, -62.0343183007696, 1.1489628558154772)\n",
      "pose vision:  (180.14766206816807, -62.0179728627339, 1.1400095929476972)\n",
      "pose vision:  (180.14984276828295, -62.01938786719324, 1.1489399933586206)\n",
      "pose vision:  (180.1504776118042, -62.019814200790584, 1.1496333990882377)\n",
      "pose vision:  (180.150599950987, -62.019906100421814, 1.1496872389480117)\n",
      "pose vision:  (180.15062440752016, -62.01992466098623, 1.149691419372802)\n",
      "pose vision:  (180.15062934833475, -62.019928405723014, 1.1496917439641248)\n",
      "pose vision:  (180.1506303469876, -62.01992916230347, 1.149691769167192)\n",
      "pose vision:  (180.1438297116417, -62.01499345162452, 1.140066189815836)\n",
      "pose vision:  (180.14227522107106, -62.01383419326885, 1.139318806544539)\n",
      "pose vision:  (180.14196736893803, -62.013594476223034, 1.13926077556959)\n",
      "pose vision:  (180.14190542991295, -62.013546059176754, 1.1392562697234156)\n",
      "pose vision:  (180.14867530150622, -62.018501273105976, 1.1488815011728368)\n",
      "pose vision:  (181.0160367441095, -61.933529466701486, 1.1527457322608328)\n",
      "pose vision:  (180.26041925445634, -62.08854529827255, 1.1499288977264595)\n",
      "pose vision:  (180.16450225349544, -62.03079817454303, 1.140084601789999)\n",
      "pose vision:  (181.01911689618913, -61.935838897930275, 1.1520626922901442)\n",
      "pose vision:  (180.25195506391043, -62.084457057296945, 1.1402502814133317)\n",
      "pose vision:  (181.0398488689672, -61.95289539382794, 1.1615333006263446)\n",
      "pose vision:  (180.24675142596152, -62.08266407322834, 1.1309290276094623)\n",
      "pose vision:  (181.03020485541936, -61.94191240050037, 1.1414602524103756)\n",
      "pose vision:  (180.45454379877768, -60.259611244866505, 1.170675816176514)\n",
      "pose vision:  (179.64802694514185, -57.37879695226719, 1.2068409073628104)\n",
      "pose vision:  (94.26787031911236, -35.97053144336356, 2.5446186920559093)\n",
      "pose vision:  (167.591333500659, -49.7208689802577, 1.2512269446211945)\n"
     ]
    }
   ],
   "source": [
    "# should not exist here\n",
    "# q_x = q_y = 0.00017\n",
    "# q_theta = 0.00098\n",
    "# q_vx = q_vy = 0.0178\n",
    "\n",
    "r_x = r_y = 5.396e-05\n",
    "r_theta = 9.008e-05\n",
    "r_vx = r_vy = 1.213\n",
    "\n",
    "C_not_obstructed = np.eye(5)\n",
    "\n",
    "C_obstructed = np.array([[0,0,0,0,0],\n",
    "                         [0,0,0,0,0],\n",
    "                         [0,0,0,0,0],\n",
    "                         [0,0,0,1,0],\n",
    "                         [0,0,0,0,1],\n",
    "                         ])\n",
    "\n",
    "R_obstructed = np.array([[np.inf,0,0,0,0],\n",
    "                         [0,np.inf,0,0,0],\n",
    "                         [0,0,np.inf,0,0],\n",
    "                         [0,0,0,1.21,0],\n",
    "                        [0,0,0,0,1.21]])\n",
    "\n",
    "\n",
    "R_not_obstructed = np.array([\n",
    "                        [r_x,0,0,0,0],\n",
    "                        [0,r_y,0,0,0],\n",
    "                        [0,0,r_theta,0,0],\n",
    "                        [0,0,0, r_vx,0],\n",
    "                        [0,0,0,0, r_vy]\n",
    "                        ])\n",
    "\n",
    "# Should not exist\n",
    "# Q = np.array([\n",
    "#             [q_x,0,0,0,0],\n",
    "#             [0,q_y,0,0,0],\n",
    "#             [0,0,q_theta,0,0],\n",
    "#             [0,0,0,q_vx,0],\n",
    "#             [0,0,0,0,q_vy]\n",
    "#             ])\n",
    "\n",
    "async def main():\n",
    "    global x_odom, y_odom, theta_odom, is_cam_obstructed, thymio_position_vision,is_thymio_found, path, search_goal, search_start, map, u_state_prev,  sigma_post_odom, button_forward, is_obstacle, count_odom\n",
    "\n",
    "    is_obstacle = False\n",
    "\n",
    "    # VISION\n",
    "    # open the default camera\n",
    "    cam = cv2.VideoCapture(1, cv2.CAP_DSHOW)\n",
    "    if not cam.isOpened():\n",
    "            print(\"Error: Could not open camera.\")\n",
    "\n",
    "    # convert camera resolution from 1920x1080 to 1024x768\n",
    "    cam.set(cv2.CAP_PROP_FRAME_WIDTH, 1024)\n",
    "    cam.set(cv2.CAP_PROP_FRAME_HEIGHT, 768)\n",
    "\n",
    "    map, search_goal, search_start, theta_vision_init, old_corners = initialise_setup(cam) # in (x, y, theta_rad)_v\n",
    "\n",
    "    # INIT POUR KALMAN\n",
    "    x_vision, y_vision = thymio_transform_pose(search_start, search_start) # in (x, y, theta_rad)_t\n",
    "    \n",
    "    mu_post_vision = np.array([[x_vision], [y_vision], [theta_vision_init], [0], [0]])\n",
    "    # mu_post_odom = np.array([[0], [0], [theta_vision_init], [0], [0]]) # should not be like this\n",
    "    mu_post_odom = np.array([[0], [0], [0], [0], [0]])\n",
    "\n",
    "    u_state_prev = np.transpose(np.array([0,0]))\n",
    "    sigma_post_odom = np.array([\n",
    "                                [0,0,0,0,0],\n",
    "                                [0,0,0,0,0],\n",
    "                                [0,0,0,0,0],\n",
    "                                [0,0,0,0,0],\n",
    "                                [0,0,0,0,0]\n",
    "                                ])\n",
    "    \n",
    "    sigma_post_vision = np.array([\n",
    "                                [0,0,0,0,0],\n",
    "                                [0,0,0,0,0],\n",
    "                                [0,0,0,0,0],\n",
    "                                [0,0,0,0,0],\n",
    "                                [0,0,0,0,0]\n",
    "                                ])\n",
    "\n",
    "    map_image = np.float32(map.copy())\n",
    "    map_image[map == -1] = 255\n",
    "    cv2.imwrite(\"blue filter.jpg\", map_image)\n",
    "\n",
    "    path = path_planning_a_star(map, search_start, search_goal)\n",
    "    path = path_finalizing(path, search_start, SCALING_FACTOR) # in (x, y, theta_rad)_t\n",
    "    path = [(float(x), float(y), float(theta)) for x, y, theta in path]\n",
    "    \n",
    "    x_odom = y_odom = 0\n",
    "    #theta_odom = theta_vision_init # should not be like this\n",
    "    theta_odom = 0\n",
    "    count_odom = True\n",
    "    \n",
    "    while True:\n",
    "        start_time = time.time()\n",
    "        ret, image = cam.read()\n",
    "        await node.wait_for_variables({\n",
    "            \"motor.left.speed\",\n",
    "            \"motor.right.speed\",\n",
    "            \"button.center\",\n",
    "            \"button.forward\",\n",
    "            \"prox.horizontal\",\n",
    "            \"acc\"\n",
    "        })\n",
    "\n",
    "        # VARIABLES\n",
    "        motor_left = node[\"motor.left.speed\"]\n",
    "        motor_right = node[\"motor.right.speed\"]\n",
    "        button_center = node[\"button.center\"]\n",
    "        button_forward = node[\"button.forward\"]\n",
    "        prox_horizontal = node[\"prox.horizontal\"] \n",
    "        acceleration = node[\"acc\"]\n",
    "\n",
    "        bird_image = project_image(image, old_corners)\n",
    "        bird_image = show_path(bird_image.copy(), path, search_start)\n",
    "        is_cam_obstructed = is_camera_obstructed(image)\n",
    "\n",
    "        # FILTERING KALMAN\n",
    "        is_thymio_found, thymio_position_vision, theta_vision = get_thymio_pose(bird_image)\n",
    "        theta_vision = normalize_angle(theta_vision)\n",
    "        x_vision, y_vision = thymio_transform_pose(thymio_position_vision, search_start) # in (x, y, theta_rad)_t\n",
    "\n",
    "        if not is_cam_obstructed:\n",
    "            mu_post_vision, sigma_post_vision = EKF(x_vision, y_vision, theta_vision, 0, 0, mu_post_vision, u_state_prev, sigma_post_vision, R_not_obstructed, C_not_obstructed, False)\n",
    "            pose = (float(mu_post_vision[0, 0]), float(mu_post_vision[1, 0]), float(normalize_angle(mu_post_vision[2, 0])))\n",
    "            print('pose vision: ', pose)\n",
    "            bird_image = show_kalman(bird_image, pose, sigma_post_vision, search_start)\n",
    "            \n",
    "\n",
    "        else:\n",
    "            # Should be like this\n",
    "            # if count_odom:\n",
    "            #     x_odom = mu_post_vision[0, 0]\n",
    "            #     y_odom =  mu_post_vision[1, 0]\n",
    "            #     theta_odom = normalize_angle(mu_post_vision[2, 0])\n",
    "            #     mu_post_odom = mu_post_vision\n",
    "            #     sigma_post_odom = sigma_post_vision\n",
    "\n",
    "            #     print(count_odom)\n",
    "            #     print('inside cound', x_odom, y_odom, theta_odom)\n",
    "            #     count_odom = False\n",
    "\n",
    "            x_odom, y_odom, theta_odom, _, _= odometrie(x_odom,y_odom,theta_odom, motor_right, motor_left)\n",
    "            theta_odom = normalize_angle(theta_odom)\n",
    "            mu_post_odom, sigma_post_odom = EKF(x_odom, y_odom, theta_odom, motor_right, motor_left, mu_post_odom, u_state_prev, sigma_post_odom, R_obstructed, C_obstructed, True)\n",
    "            pose = (float(mu_post_odom[0, 0]), float(mu_post_odom[1, 0]), normalize_angle(float(mu_post_odom[2, 0])))\n",
    "            bird_image = show_kalman(bird_image, pose, sigma_post_odom, search_start)\n",
    "\n",
    "        \n",
    "        cv2.imshow(\"bird image\", bird_image)\n",
    "        cv2.imwrite(\"path_image.png\", bird_image)\n",
    "\n",
    "        # LOCAL AVOIDANCE\n",
    "        motor_speed, is_obstacle = local_avoidance(prox_horizontal)\n",
    "        is_kidnapped = kidnapping(acceleration, prox_horizontal)\n",
    "\n",
    "        if is_kidnapped:\n",
    "             await node.set_variables(motors(0, 0))\n",
    "        elif is_obstacle:\n",
    "            await node.set_variables(motors(motor_speed[0], motor_speed[1]))\n",
    "        else:\n",
    "            u_state_prev[0],u_state_prev[1] = await motion_control(pose, path)\n",
    "            \n",
    "        # THYMIO DIRECT CONTROL\n",
    "        \n",
    "        if button_forward == 1:\n",
    "            await node.set_variables(motors(100, 100))\n",
    "\n",
    "        if button_center == 1:\n",
    "            await node.set_variables(motors(0, 0))\n",
    "            x_odom = y_odom = theta_odom = 0\n",
    "            break\n",
    "\n",
    "    cam.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    time_diff = time.time() - start_time\n",
    "\n",
    "    if time_diff < DELTA_T:\n",
    "        await client.sleep(DELTA_T - time_diff)\n",
    "\n",
    "await main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
